 Novel color via stimulation of individual photoreceptors at population scale | Science Advances                                

[Skip to main content](#main-content-focus)

Advertisement

*   [news](/news)
*   [careers](/careers)
*   [commentary](/commentary)
*   [Journals](/journals)

 [![](/pb-assets/images/styleguide/logo-dark-1672180581427.svg) ![Science](/pb-assets/images/styleguide/logo-1672180580750.svg)](/)

*   [](#header-quick-search-wrapper "Search")
*   [Log in](/action/ssostart?redirectUri=%2Fdoi%2F10.1126%2Fsciadv.adu1052)
    
*   [Become A Member](https://promo.aaas.org/science/join/?CTC=SMHPJN)
    

[![journal-menu-img](/cms/asset/8f0f8494-a4fa-41f2-b5a9-3a363c2f367c/science.2025.388.issue-6744.cover.gif)

science

](/journal/science "Science")

[![journal-menu-img](/cms/asset/1b800672-cf16-468d-a991-fda35720bb10/sciadv.2025.11.issue-16.cover.gif)

science advances

](/journal/sciadv "Science")

[![journal-menu-img](/cms/asset/1c77631e-056b-416f-b006-f05f6ba4077b/sciimmunol.2025.10.issue-106.largecover.gif)

science immunology

](/journal/sciimmunol "Science")

[![journal-menu-img](/cms/asset/05d31264-a97d-4ccf-a9cc-e02a4650e15e/scirobotics.2025.10.issue-101.largecover.gif)

science robotics

](/journal/scirobotics "Science")

[![journal-menu-img](/cms/asset/5caf06fe-404d-4002-91ca-f3f6e1a7d4eb/signaling.2025.18.issue-882.largecover.gif)

science signaling

](/journal/signaling "Science")

[![journal-menu-img](/cms/asset/f23c1542-9acc-47d0-b44e-06441f4f84b0/stm.2025.17.issue-794.largecover.gif)

science translational medicine

](/journal/stm "Science")

[![journal-menu-img](/pb-assets/images/styleguide/spj-cover-1695403298717.png)

science partner journals

](https://spj.science.org/ "Science Partner Journals")

[](#journal-menu)

Quick Search anywhere

Enter Search Term

Quick Search in Journals

Enter Search Term

Quick Search in Journals

Enter Search Term

Quick Search in Journals

Enter Search Term

Quick Search in Journals

Enter Search Term

Quick Search in Journals

Enter Search Term

Quick Search in Journals

Enter Search Term

Searching:

Anywhere

[Anywhere](#quick-search-form-9ea9b667-c3ce-40d1-870b-f7e001bbac01-0)[Science](#quick-search-form-9ea9b667-c3ce-40d1-870b-f7e001bbac01-1)[Science Advances](#quick-search-form-9ea9b667-c3ce-40d1-870b-f7e001bbac01-2)[Science Immunology](#quick-search-form-9ea9b667-c3ce-40d1-870b-f7e001bbac01-3)[Science Robotics](#quick-search-form-9ea9b667-c3ce-40d1-870b-f7e001bbac01-4)[Science Signaling](#quick-search-form-9ea9b667-c3ce-40d1-870b-f7e001bbac01-5)[Science Translational Medicine](#quick-search-form-9ea9b667-c3ce-40d1-870b-f7e001bbac01-6)

[Advanced Search](/search/advanced) Search

###### Trending Terms:

*   [cancer](/action/doSearch?AllField=cancer)
*   [climate](/action/doSearch?AllField=climate)
*   [artificial intelligence](/action/doSearch?AllField=artificial intelligence)
*   [postdoc](/action/doSearch?AllField=postdoc)
*   [aging](/action/doSearch?AllField=aging)

[](#)

*   [Log In](/action/ssostart?redirectUri=/doi/10.1126/sciadv.adu1052)

[Become A Member](/action/clickThrough?id=501021&url=https%3A%2F%2Fpromo.aaas.org%2Fscience%2Fjoin%2F%3FCTC%3DSMLDJN&loc=%2Fdoi%2F10.1126%2Fsciadv.adu1052&pubId=42126383&placeholderId=501021&productId=501021)

*   [Donate](/news/donate?intcmp=sitemenu-donate&utm_id=recB0g01G48yiGSBD)

Quick Search anywhere

Enter Search Term

[science.org](/)

*   [news](/news)
*   [careers](/careers)
*   [commentary](/commentary)
*   [Journals](/journals)
*   [science](/journal/science)
*   [science advances](/journal/sciadv)
    *   [Current Issue](/toc/sciadv/current)
    *   [First release papers](/toc/sciadv/0/0)
    *   [Archive](/loi/sciadv)
    *   [About](#)
        *   [About Science Advances](/content/page/science-advances-mission-and-scope)
        *   [Mission & Scope](/content/page/science-advances-mission-and-scope)
        *   [Editorial Board](/content/page/science-advances-editorial-board)
        *   [Editorial Working Groups](/content/page/science-advances-editorial-working-groups)
        *   [Information for Authors](/content/page/science-advances-information-authors)
        *   [Information for Reviewers](/content/page/science-advances-information-reviewers)
        *   [Editorial Policies](/content/page/science-journals-editorial-policies)
        *   [Licensing and Charges](/content/page/science-advances-licensing-and-charges)
        *   [Journal Metrics](/content/page/journal-metrics-overview)
        *   [Frequently Asked Questions](/content/page/science-advances-frequently-asked-questions)
        *   [Staff](/content/page/science-advances-staff)
        *   [Contact Us](/content/page/science-advances-contact-information)
        *   [TOC Alerts and RSS Feeds](/content/page/email-alerts-and-rss-feeds)
*   [science immunology](/journal/sciimmunol)
*   [science robotics](/journal/scirobotics)
*   [science signaling](/journal/signaling)
*   [science translational medicine](/journal/stm)
*   [science partner journals](https://spj.science.org/)

*   [Custom publishing](/custom-publishing)
*   [prizes and awards](/content/page/prizes-and-awards)
*   [newsletters](/topic/article-type/scienceadviser?intcmp=menu-adviserfeed&utm_id=recI11u4srAIiGNLZ)
*   [collections](/collections)
*   [videos](/videos)
*   [podcasts](/podcasts)
*   [blogs](/blogs)
*   [visualizations](/content/page/visualizations)

*   [authors & reviewers](/content/page/contributing-science-family-journals)
*   [librarians](/content/page/librarian-portal)
*   [advertisers](https://advertising.sciencemag.org/)
*   [about](/content/page/aboutus)
*   [help](/content/page/help)

*   [](https://www.facebook.com/ScienceMagazine)
*   [](https://twitter.com/sciencemagazine)
*   [](https://www.instagram.com/ScienceMagazine)
*   [](https://www.youtube.com/user/ScienceMag)
*   [](/content/page/email-alerts-and-rss-feeds)
*   [](https://mp.weixin.qq.com/s?__biz=MzI3NDY3NzQ2Mg==&mid=100002815&idx=1&sn=2949c025a553ac718b9612a0473b9f60&chksm=6b1120465c66a9508b01eaef1589b15d440e50b189106c8c594de8c6471f696a978de952fb15&mpshare=1&scene=1&srcid=0716JJQ5V4cKbgMMsya2MQ0n&sharer_sharetime=)

![AAAS Science Logo](/pb-assets/images/styleguide/logo-1672180580750.svg)

*   [Terms of Service](/content/page/terms-service)
*   [Privacy Policy](/content/page/privacy-policy)
*   [Accessibility](/content/page/accessibility)

[](#header-side-menu)

[![logo](/pb-assets/images/logos/sciadv-logo-1620488349693.svg)](/journal/sciadv)

*   [Current Issue](/toc/sciadv/current)
*   [First release papers](/toc/sciadv/0/0)
*   [Archive](/loi/sciadv)
*   [About](#)
    
    [About Science Advances](/content/page/science-advances-mission-and-scope) [Mission & Scope](/content/page/science-advances-mission-and-scope) [Editorial Board](/content/page/science-advances-editorial-board) [Editorial Working Groups](/content/page/science-advances-editorial-working-groups) [Information for Authors](/content/page/science-advances-information-authors) [Information for Reviewers](/content/page/science-advances-information-reviewers) [Editorial Policies](/content/page/science-journals-editorial-policies) [Licensing and Charges](/content/page/science-advances-licensing-and-charges) [Journal Metrics](/content/page/journal-metrics-overview) [Frequently Asked Questions](/content/page/science-advances-frequently-asked-questions) [Staff](/content/page/science-advances-staff) [Contact Us](/content/page/science-advances-contact-information) [TOC Alerts and RSS Feeds](/content/page/email-alerts-and-rss-feeds)
    

*   [Submit manuscript](https://cts.sciencemag.org/)
*   [More](About)
    
    *   [Current Issue](/toc/sciadv/current)
    *   [First release papers](/toc/sciadv/0/0)
    *   [Archive](/loi/sciadv)
    *   [About](#)
        
        [About Science Advances](/content/page/science-advances-mission-and-scope)[Mission & Scope](/content/page/science-advances-mission-and-scope)[Editorial Board](/content/page/science-advances-editorial-board)[Editorial Working Groups](/content/page/science-advances-editorial-working-groups)[Information for Authors](/content/page/science-advances-information-authors)[Information for Reviewers](/content/page/science-advances-information-reviewers)[Editorial Policies](/content/page/science-journals-editorial-policies)[Licensing and Charges](/content/page/science-advances-licensing-and-charges)[Journal Metrics](/content/page/journal-metrics-overview)[Frequently Asked Questions](/content/page/science-advances-frequently-asked-questions)[Staff](/content/page/science-advances-staff)[Contact Us](/content/page/science-advances-contact-information)[TOC Alerts and RSS Feeds](/content/page/email-alerts-and-rss-feeds)
        
    *   [Submit manuscript](https://cts.sciencemag.org/)
    

[GET OUR E-ALERTS](/action/showPreferences?menuTab=Alerts)

Main content starts here

[Home](https://www.science.org/)[Science Advances](/journal/sciadv)[Vol. 11, No. 16](/toc/sciadv/11/16)Novel color via stimulation of individual photoreceptors at population scale

[Back To Vol. 11, No. 16](/toc/sciadv/11/16)

Open access

Research Article

COGNITIVE NEUROSCIENCE

Share on

*   
*   
*   
*   
*   
*   
*   
*   

# Novel color via stimulation of individual photoreceptors at population scale

[James Fong](#con1) [https://orcid.org/0009-0009-5106-8404](https://orcid.org/0009-0009-5106-8404), [Hannah K. Doyle](#con2) [https://orcid.org/0009-0003-3101-8588](https://orcid.org/0009-0003-3101-8588), \[...\] , [Congli Wang](#con3) [https://orcid.org/0000-0003-3112-2820](https://orcid.org/0000-0003-3112-2820), [Alexandra E. Boehm](#con4), \[...\] , [Sofie R. Herbeck](#con5) [https://orcid.org/0009-0000-2044-7895](https://orcid.org/0009-0000-2044-7895), [Vimal Prabhu Pandiyan](#con6) [https://orcid.org/0000-0002-6996-0355](https://orcid.org/0000-0002-6996-0355), [Brian P. Schmidt](#con7) [https://orcid.org/0000-0003-3460-0605](https://orcid.org/0000-0003-3460-0605), [Pavan Tiruveedhula](#con8) [https://orcid.org/0009-0001-5672-4046](https://orcid.org/0009-0001-5672-4046), [John E. Vanston](#con9), \[...\] , [William S. Tuten](#con10) [https://orcid.org/0000-0003-3856-017X](https://orcid.org/0000-0003-3856-017X), [Ramkumar Sabesan](#con11) [https://orcid.org/0000-0003-0895-7037](https://orcid.org/0000-0003-0895-7037), [Austin Roorda](#con12) [https://orcid.org/0000-0002-3785-0848](https://orcid.org/0000-0002-3785-0848), and [Ren Ng](#con13) [https://orcid.org/0009-0007-2207-021X](https://orcid.org/0009-0007-2207-021X) [ren@berkeley.edu](mailto:ren@berkeley.edu)+10 authors +8 authors +3 authors fewer[Authors Info & Affiliations](#tab-contributors)

Science Advances

18 Apr 2025

Vol 11, Issue 16

[DOI: 10.1126/sciadv.adu1052](https://doi.org/10.1126/sciadv.adu1052)

[

###### PREVIOUS ARTICLE

Hippocampal perineuronal net degradation identifies prefrontal and striatal circuits involved in schizophrenia-like changes in marmosets

Previous](/doi/10.1126/sciadv.adu0975 "Hippocampal perineuronal net degradation identifies prefrontal and striatal circuits involved in schizophrenia-like changes in marmosets")[

###### NEXT ARTICLE

Earlier onset of chemotherapy-induced neuropathic pain in females by ICAM-1–mediated accumulation of perivascular macrophages

Next](/doi/10.1126/sciadv.adu2159 "Earlier onset of chemotherapy-induced neuropathic pain in females by ICAM-1–mediated accumulation of perivascular macrophages")

[

23,937](# "Metrics")

### Metrics

#### Total Downloads23,937

*   Last 6 Months23,937
*   Last 12 Months23,937

[View all metrics](?doi=10.1126/sciadv.adu1052#core-collateral-metrics "Go to metrics page")

[Notifications](/action/addCitationAlert?doi=10.1126%2Fsciadv.adu1052)[Bookmark](/personalize/addFavoritePublication?doi=10.1126%2Fsciadv.adu1052)

[](#tab-citations "CITE")

[](/doi/reader/10.1126/sciadv.adu1052 "PDF")

*   Contents
    
    *   [Abstract](#abstract)
    *   [INTRODUCTION](#sec-1)
    *   [RESULTS](#sec-2)
    *   [DISCUSSION](#sec-3)
    *   [MATERIALS AND METHODS](#sec-4)
    *   [Acknowledgments](#acknowledgments)
    *   [Supplementary Materials](#supplementary-materials)
    *   [REFERENCES AND NOTES](#bibliography)
    *   [eLetters (0)](#elettersSection)
    

*   [Information & Authors](#core-collateral-info)
*   [Metrics & Citations](#core-collateral-metrics)
*   [View Options](#core-collateral-fulltext-options)
*   [References](#core-collateral-references)
*   [Figures](#core-collateral-figures)
*   [Tables](#core-collateral-tables)
*   [Media](#core-collateral-media)
*   [Share](#core-collateral-share)

## Abstract

We introduce a principle, Oz, for displaying color imagery: directly controlling the human eye’s photoreceptor activity via cell-by-cell light delivery. Theoretically, novel colors are possible through bypassing the constraints set by the cone spectral sensitivities and activating M cone cells exclusively. In practice, we confirm a partial expansion of colorspace toward that theoretical ideal. Attempting to activate M cones exclusively is shown to elicit a color beyond the natural human gamut, formally measured with color matching by human subjects. They describe the color as blue-green of unprecedented saturation. Further experiments show that subjects perceive Oz colors in image and video form. The prototype targets laser microdoses to thousands of spectrally classified cones under fixational eye motion. These results are proof-of-principle for programmable control over individual photoreceptors at population scale.

#### SIGN UP FOR THE AWARD-WINNING *SCIENCE*ADVISER NEWSLETTER

The latest news, commentary, and research, free to your inbox daily

[Sign up](/action/clickThrough?id=501069&url=%2Fcontent%2Fpage%2Fscienceadviser%3Fintcmp%3Dadvint-adviser%26utm_id%3DrectXwbHbZxMmztQo&loc=%2Fdoi%2F10.1126%2Fsciadv.adu1052&pubId=42126383&placeholderId=501023&productId=501024)

## INTRODUCTION

We introduce a new principle for displaying color, which we call Oz: optically stimulating individual photoreceptor cells on the retina at population scale to directly control their activation levels. In principle, arbitrary colored visual imagery can be displayed by this cell-by-cell approach, but doing so requires exquisite precision in reproducing the dynamic stimulation levels at each photoreceptor as imagery traverses the retina under eye movements (see [Fig. 1Opens in image viewer](#F1)). As proof of principle, we perform human subject experiments on a prototype Oz system that stimulates thousands of retinal cone cells.

[![](/cms/10.1126/sciadv.adu1052/asset/676ebf10-6471-4c5a-9b79-dfe102d331a1/assets/images/large/sciadv.adu1052-f1.jpg)](#F1)

Fig. 1. Overview of principle and prototype system.

(**A**) System inputs. (i) Retina map of 103 cone cells preclassified by spectral type ([*7*](#core-R7)). (ii) Target visual percept (here, a video of a child, see movie S1 at 1:04). (iii) Infrared cellular-scale imaging of the retina with 60-frames-per-second rolling shutter. Fixational eye movement is visible over the three frames shown. (**B**) System outputs. (iv) Real-time per-cone target activation levels to reproduce the target percept, computed by: extracting eye motion from the input video relative to the retina map; identifying the spectral type of every cone in the field of view; computing the per-cone activation the target percept would have produced. (v) Intensities of visible-wavelength 488-nm laser microdoses at each cone required to achieve its target activation level. (**C**) Infrared imaging and visible-wavelength stimulation are physically accomplished in a raster scan across the retinal region using AOSLO. By modulating the visible-wavelength beam’s intensity, the laser microdoses shown in (v) are delivered. Drawing adapted with permission \[Harmening and Sincich ([*54*](#core-R54))\]. (**D**) Examples of target percepts with corresponding cone activations and laser microdoses, ranging from colored squares to complex imagery. Teal-striped regions represent the color “olo” of stimulating only M cones.

Collapse

[Open in viewer](#F1)

Theoretically, Oz enables display of colors that lie beyond the well-known, bounded color gamut of natural human vision ([*1*](#core-R1)). In normal color vision, any light that stimulates an M cone cell must also stimulate its neighboring L and/or S cones, because the M cone spectral response function lies between that of the L and S cones and overlaps completely with them ([*2*](#core-R2), [*3*](#core-R3)). However, Oz stimulation can by definition target light to only M cones and not L or S, which in principle would send a color signal to the brain that never occurs in natural vision. Theoretically, Oz expands the natural human color gamut to any (L, M, and S) color coordinate (see [Fig. 2Opens in image viewer](#F2)). In practice, we achieve a partial expansion of colorspace toward this theoretical maximum.

[![](/cms/10.1126/sciadv.adu1052/asset/d0883609-d37f-450f-b297-5a934fddff67/assets/images/large/sciadv.adu1052-f2.jpg)](#F2)

Fig. 2. Theoretical model of Oz color gamut as a function of fractional leak and stimulation wavelength.

(**A**) Gamut shrinks from the full *lms* chromaticity triangle to the stimulation wavelength (open circle) as the fractional light leak grows; note that this fraction depends on the intercone spacing, which varies across the retina. The colored region is the gamut of natural human colors. (**B**) Gamut varies in chromaticity, position, and shape as a function of stimulation wavelength. For readability, extra copies of the gamuts for 543 and 589 nm are drawn next to the *lm* edge.

[Open in viewer](#F2)

The closest prior art for selectively exciting M cones is targeting light to only one ([*4*](#core-R4)–[*7*](#core-R7)) or two ([*8*](#core-R8)) cones at a time. Aside from cone-targeted methods, the only other methods to selectively excite M cones use visual pre-adaptation such as bleaching L photopigment with red light before displaying green light ([*9*](#core-R9), [*10*](#core-R10)). However, such percepts rely on fleeting adaptation states and after-images, so they are difficult to measure precisely ([*9*](#core-R9), [*11*](#core-R11)). A different method called silent substitution ([*12*](#core-R12), [*13*](#core-R13)) can isolate activation changes to M cones, but requires baseline activation of the other cone classes and cannot display colors beyond the human gamut. In contrast to these approaches, our Oz prototype displays colors beyond the natural human gamut over a large enough area for color matching, for sustained durations, and within arbitrary colored imagery.

Our Oz prototype is a proof-of-principle that builds upon the cone-targeted methods ([*4*](#core-R4)–[*8*](#core-R8)) that use adaptive optics scanning light ophthalmoscopy (AOSLO) ([*14*](#core-R14)). First, adaptive optics optical coherence tomography (AO-OCT) ([*15*](#core-R15), [*16*](#core-R16)) is used to spectrally preclassify the LMS type of 103 retinal cone cells ([*17*](#core-R17)) per subject. Then, AOSLO produces Oz percepts by imaging the retina in infrared to near-invisibly track eye motion at cellular scale, and targeting 105 visible-wavelength laser microdoses per second to each cone cell. The visual field of view of the prototype is a 0.9° square centered at 4° adjacent to a gaze-fixation target.

We mapped the empirical colorspace coordinates of Oz colors in practice using formal color matching experiments ([Fig. 3Opens in image viewer](#F3)) and collected qualitative judgments of hue and saturation. These experiments confirmed that the prototype successfully displays a range of hues in Oz: e.g., from orange to yellow to green to blue-green with a 543-nm stimulating laser that ordinarily looks green. Further, color matching confirms that our attempt at stimulating only M cones displays a color that lies beyond the natural human gamut. We name this new color “olo,” with the ideal version of olo defined as pure M activation. Subjects report that olo in our prototype system appears blue-green of unprecedented saturation, when viewed relative to a neutral gray background. Subjects find that they must desaturate olo by adding white light before they can achieve a color match with the closest monochromatic light, which lies on the boundary of the gamut, unequivocal proof that olo lies beyond the gamut.

[![](/cms/10.1126/sciadv.adu1052/asset/fb500790-9651-44e9-998b-9df4d8cd125c/assets/images/large/sciadv.adu1052-f3.jpg)](#F3)

Fig. 3. Color matching of Oz colored squares produced by cone-by-cone stimulation.

(**A** to **D**) Each *lms* chromaticity triangle plots color matches for one subject with the indicated stimulation wavelength and type of matching color system (RGB projector, or tunable near-monochromatic laser and projector white). Target colors are specified as (L, M, and S) triplets, which are the relative light intensity levels directed to each cone class. Color matches to different target colors are denoted with differently colored markers. Each triangle also plots: color matches for randomly interleaved jitter control condition \[see (E) and the “Design of prototype” section\]; coordinates of the stimulation wavelength; natural color gamut of human vision; gamut of the matching color system and its whitepoint; and perceptual uncertainty ellipses for the average color matches (projected JND ellipsoid at the coordinates of the “positive” component of the color match, computed from CIELAB/ΔEab\*, scaled three times the actual size; see the “Plotting perceptual uncertainty in matching” section in Materials and Methods). Ellipses not visible are smaller than their associated markers. (**E**) Illustration of the control condition randomly interleaved into all experiments: microdose target locations are randomly jittered by two intercone spacings in Oz stimuli that are otherwise identical to the experimental condition.

Collapse

[Open in viewer](#F3)

In control experiments, Oz color matches “collapse” to the natural color of the laser, as expected, if we “jitter” the target location of each laser microdose so that it incorrectly lands on a random neighboring cell. In addition, subjects clearly perceive Oz hues in image and video form, such as an oriented red line or a rotating red dot on an olo background ([Fig. 4Opens in image viewer](#F4)), and cannot do so under the jitter control condition.

[![](/cms/10.1126/sciadv.adu1052/asset/570cf67c-7982-4046-9ca3-0b9d0965b71f/assets/images/large/sciadv.adu1052-f4.jpg)](#F4)

Fig. 4. Image and video recognition experiments.

We tested subjects’ ability to recognize image and video content consisting of Oz colors: (**A**) a 4-alternative forced choice (4-AFC) line orientation recognition task, and (**B**), a 2-AFC rotation direction task experiments. Oz stimuli consisted of equiluminant red lines and disks presented on an olo background, as depicted. The bar graphs show individual subject performance over 20 trials per condition and average accuracy across five subjects with 95% confidence intervals. In experimental conditions with Oz microdoses delivered experimentally (blue bars), subjects are able to accurately identify line orientation and rotation direction. In control-group stimuli (gray bars), where cone-targeting is compromised through jittering microdose target locations, task accuracy is reduced to guessing rate as indicated by the dashed lines.

[Open in viewer](#F4)

For any color distinct from the natural color of the stimulating laser to be perceived in Oz, our prototype system must perform high-resolution retinal imaging, high-speed tracking of eye motion, and low-latency stimulus delivery ([*18*](#core-R18)). Demonstrating colors outside of the natural human gamut in Oz is the perceptual signature that each of these system components is operating successfully in unison. This technical achievement introduces an experimental platform for visual perception with a new class of precision, programmable control, and cellular scale.

## RESULTS

### Theory of cell-by-cell color

We plot the colors generated by our Oz prototype on a Maxwell triangle ([*19*](#core-R19)) with barycentric coordinates (l,m,s)\=(L,M,S)L+M+S. This triangle displays the chromaticity (hue and saturation of a color) in two dimensions (2D), while projecting out its total activation (*L* + *M* + *S*). In these diagrams, the color-filled subregion plotted at the bottom is the natural human gamut, which spans all chromaticities achievable via ordinary spectral mixtures of light.

In theory, the full area of the chromaticity triangle itself is the fundamentally larger color gamut that is accessible via cell-by-cell stimulation in Oz, assuming idealized conditions that produce perfect localization of light to target cones. In practice, however, a fraction of the light will miss target cones and stimulate neighboring cells, causing the resulting activation pattern to shift from the intended Oz color toward the laser’s natural color.

The effect of such stray cone activation on achievable colors is predicted in [Fig. 2Opens in image viewer](#F2). The key factors are: the point-spread function (PSF) of the laser microdoses on the retina relative to the spacing of cone cells, the cone’s spatial light gathering function ([*20*](#core-R20)–[*22*](#core-R22)), errors in microdose targeting during eye movement, the retina’s L:M:S cone proportion, and the stimulating wavelength (details in the “Theoretical modeling of Oz color gamuts” section in Materials and Methods).

[Figure 2AOpens in image viewer](#F2) illustrates how fractional light leak would affect the gamut of achievable Oz chromaticities. In theory, a diffraction-limited PSF would enable Oz to address nearly all possible chromaticities in the *lms* triangle when stimulating the retina at 4° eccentricity, as shown, but not in the foveola where cone cells are smallest. In practice, the total leakage of light includes more than diffraction due to factors such as residual aberrations after adaptive optics focusing and errors in microdose targeting due to computational latency during eye movement. Measuring these factors directly is challenging, but a best fit of the model shown in [Fig. 2AOpens in image viewer](#F2) against the experimental color matching data in the upcoming “Color matching experiments” section suggests that, of the light captured by cones, one-third is confined to the target cell and two-thirds is captured by neighboring cones. Despite this unintended light leak, this level of accuracy succeeds in displaying color beyond the natural human gamut in our Oz prototype.

[Figure 2BOpens in image viewer](#F2) illustrates how the stimulating wavelength would affect the gamut of achievable Oz chromaticities. The shape of this gamut reflects the relative responses of the L, M, and S cone cells at a given stimulating wavelength, forming a triangle, line, or single point depending on the number of cone types that respond at that wavelength.

### Design of prototype

We build our Oz prototype on an AOSLO ([*14*](#core-R14)) that simultaneously images and stimulates the retina with a raster scan of near-diffraction-limited laser light over a 0.9° square field of view. Using nearly invisible infrared light to image the retina, we can track the eye’s motion in real time. We compensate for this motion and deliver pulses of visible-wavelength laser light dynamically targeted at each cone cell within the field of view. These laser microdoses are delivered at a rate of 105 per second to a population of 103 cones.

To achieve an intended LMS activation through cone-targeted stimulation, the spectral type of each cone must be known. In a preparatory step, cone cells are classified by spectral type in the subject’s retina using recently developed optoretinography techniques in an AO-OCT system ([*15*](#core-R15), [*23*](#core-R23)). In this study, we use a classified region containing 1000 to 2000 cones located near 4° eccentricity from the foveola.

We show Oz stimuli to human subjects and perform the following experiments: color matching of uniform Oz color squares and image/video recognition experiments. All Oz stimuli are presented within the 0.9° square field of view, 4° adjacent to a gaze fixation point, so that the stimulated area falls within the classified region of retina. As a control condition, stimuli are randomly repeated with microdose delivery intentionally compromised. During these control trials, each microdose is “jittered” randomly so that it lands two cones away from the target.

### Color matching experiments

We conduct color matching experiments to formally measure the chromaticity coordinates of Oz colors. Two different stimulation wavelengths are tested: 488 nm, which can activate all three L, M, and S cone types, and 543 nm, which is near the peak of L and M, and only minimally activates S. We use two different color matching systems: first, a red-green-blue (RGB) projector, and second, a near-monochromatic laser of tunable wavelength that can be mixed with white projector light. The latter can produce colors that lie on the edge of the natural human gamut, eliminating ambiguity as to whether our attempts to display olo truly lie beyond the natural human gamut. During a color matching trial, the subject sees 0.9° squares of Oz and controllable color, coincident in space and alternating in time, so that subjects must judge match equality using the same patch of retina, eliminating effects from differences in adaptation across the retina. As usual with color matching ([*1*](#core-R1)), subjects can add light to the Oz color (so-called “negative” light) if necessary to achieve an exact color match; its color coordinates are subtracted from those of the controllable square to calculate the matched color. Subjects are also prompted to qualitatively name the hue and rate the saturation (scale of 1 to 4) of the squares of Oz color and controllable color.

[Figure 3Opens in image viewer](#F3) graphs results of the color matching experiments. Five subjects performed 222 color matches. We highlight four observations.

First, Oz colors form a triangle around the stimulation wavelength for 488 nm ([Fig. 3BOpens in image viewer](#F3)), and a line of colors for 543 nm ([Fig. 3AOpens in image viewer](#F3)), consistent with theory in the “Theory of cell-by-cell color” section. Second, the jitter control condition causes the color to “collapse” toward the stimulation wavelength, as expected.

Third, the variance in matching *lms* chromaticity increases with the distance of Oz colors from the gamut of the color matching system. This trend is consistent with the geometric analysis in the “Plotting perceptual uncertainty in matching” section in Materials and Methods explaining why perceptual uncertainty in chromaticity increases when light must be added to the test color to achieve a match.

Fourth, [Fig. 3COpens in image viewer](#F3) provides unequivocal confirmation that olo lies beyond the natural human gamut. In these matches, all subjects found it necessary to desaturate olo with projector white in order to match the (near) monochromatic colors shown, which lie on the boundary of the natural human gamut. These matching monochromatic wavelengths, from 501 to 512 nm, are the most saturated teal hues for normal color vision under the test subjects’ viewing conditions.

Subjects’ qualitative hue naming and saturation ratings corroborate these quantitative results, although the Abney effect \[a shift in hue with saturation ([*24*](#core-R24))\] opens the possibility that the hue of the wavelength at best match may not exactly represent the hue of the undiluted olo color. Color names volunteered for olo include “teal,” “green,” “blue-greenish,” and “green, a little blue.” Subjects consistently rate olo’s saturation as 4 of 4, compared to an average rating of 2.9 for the near-monochromatic colors of matching hue shown in [Fig. 3COpens in image viewer](#F3).

### Image and video recognition experiments

We design image and video recognition experiments to probe the ability of human subjects to understand images rendered in Oz. We use four-alternative forced choice (4-AFC) and 2-AFC tasks where subjects can only succeed using hue information created through accurate Oz stimulation. In the 4-AFC task, subjects must identify the orientation of a line in an image. In the 2-AFC task, subjects must detect the rotation direction in a video of a moving disk. In these stimuli, the lines and disks are rendered as red (all-L cone) on an olo background (all-M cone), delivered using a stimulating wavelength of 543 nm. A calibration step is performed to ensure that the foreground and background are equiluminant (see the “Image and video recognition experiments” section in Materials and Methods), so that in the jitter control condition, all hue and luminance cues are removed and the task reduces to guessing.

[Figure 4Opens in image viewer](#F4) plots the results of the 4-AFC line orientation task and the 2-AFC rotation direction task. In the experimental condition, subjects are able to reliably detect both line orientation and motion direction (blue bars). In the jitter control condition, subjects’ performance is reduced to guessing for both tasks (gray bars). Qualitatively, subjects report seeing red or orange lines and disks on a blue-green or green background, when the task was easy, compared with a yellow-green square, when they were forced to guess. The former correlates directly with accurate Oz microdose deliveries, and the latter with the jitter control condition, where only the natural color of the 543-nm light should be perceived.

## DISCUSSION

All color reproduction technology today, including RGB displays and CMYK printers, is based on spectral metamerism, producing light of a spectral power distribution that causes the same activation level as a target color for each cone type in the retina. This approach dates back to at least 1861, when Maxwell gave a live demonstration at the Royal Institution of superimposing red, green, and blue images to produce the appearance of full-color images to human observers ([*25*](#core-R25)).

The Oz principle of color reproduction introduced in this paper is fundamentally different, and can be thought of as spatial metamerism in the sense that it is based on shaping the spatial distribution of light on the retina rather than its spectral distribution. Unlike conventional metamerism that requires at least three light primaries, we showed that spatial metamerism can produce a range of colors from a single monochromatic light (e.g., 543 nm laser). In addition, spatial metamerism enables fundamentally new colors, such as olo, that cannot be produced by conventional metamerism.

The required control of photoreceptor activations at population scale is technically challenging, and our experiments are limited to a 0.9° square field of view centered at 4° eccentricity, which requires gaze fixation. Enlarging Oz to an apparent N° square field-of-view and allowing subjects to gaze freely presents substantial technical challenges. It would require spectral classification of the central 2N° square patch of retina \[classification has progressed to within 0.3° eccentricity, but not yet to the smallest cells in the fovea ([*23*](#core-R23), [*26*](#core-R26))\]. It would require improving optical focus and spatiotemporal accuracy to achieve diffraction-limited microdoses within each cell, while allowing saccadic eye motion within the video field of view. It would also require scaling up; for example, to 4·104 cones and 107 microdoses per second for a 2° × 2° “free-gaze” Oz system.

Spatial metamerism requires highly dynamic spatiotemporal patterns of activation on the retina. For example, viewing a uniform square of color, corresponding to a constant ratio of L, M, and S activation, actually represents dynamic switching on and off of each cone’s activation as it enters and exits the boundary of the square during fixational eye drift (e.g., see movie S1 at 1:55). In our color matching experiments, such switches in activation occur on the order of 1000 times per second. In contrast, simply stabilizing an unchanging activation level at all cones results in the color percept rapidly fading to become invisible (<10 s), consistent with well-known Troxler fading. The dynamism of the spatiotemporal pattern of activation increases markedly when considering general image and video percepts (e.g., see movie S1 at 1:04), where fine image details move across a cone during eye movement and cause activation levels to fluctuate on the order of 105 times per second across the stimulation area. Reproducing such patterns in Oz requires fine-grained and complex programmability of each cell’s microdose intensity, and can be thought of as extending computer graphics and virtual reality from screen pixels down to the level of individual photoreceptors.

Oz represents a new class of experimental platform for vision science and neuroscience, which strives for complete control of the first neural layer to the brain, programmability of every photoreceptor’s activation at every point in time. Our prototype is an advance toward this class of neural control, and we demonstrate its ability to accurately deliver microdoses to target cones despite the challenges presented by constant fixational eye motion and the optical aberrations of the eye. When Oz microdoses are intentionally “jittered” by just a few microns, subjects perceive the stimulating laser’s natural color. When these same Oz microdoses are delivered accurately, subjects can be made to perceive different colors of the rainbow, unprecedented colors beyond the natural human gamut, and imagery like brilliant red lines or rotating dots on an olo background.

This new class of programmable platform will enable diverse new experiments. For example, Oz can support systematic probing of phenomena such as the threshold at which a small number of cones begin to contribute to a stable color percept ([*4*](#core-R4), [*5*](#core-R5), [*7*](#core-R7), [*27*](#core-R27), [*28*](#core-R28)), or the nonlinear function of a retinal ganglion cell’s response to cone activations in its receptive field ([*29*](#core-R29), [*30*](#core-R30)). Oz can reproduce and then enable programmable “micro-adjustments” to probe the cone activations underlying visual phenomena that operate near the limits of visual perception, such as the two colored-line illusion ([*31*](#core-R31)) or visual loss with high levels of cone dropout ([*32*](#core-R32), [*33*](#core-R33)). More ambitiously, Oz can be programmed to probe the plasticity of human color vision. For example, gene therapy has been used to add a third cone type in adult squirrel monkeys, producing trichromatic color vision behavior ([*34*](#core-R34)). Analogously, Oz can program signals to the human brain as if a subset of cones were filled with a new photopigment type, allowing for probing of the qualitative color experience which could not be revealed by the results of the study done in squirrel monkeys. Such an approach can flexibly probe neural plasticity to boosting color dimensionality ([*35*](#core-R35)) in humans, such as attempting to elicit full trichromatic color vision in a red-green colorblind person, or eliciting tetrachromacy in a human trichromat.

## MATERIALS AND METHODS

### Human subjects

Five subjects were recruited for this experiment \[subject number, age, sex, L:M:S ratio, center-to-center cone spacing at 4°\]: \[10001R, 40, M, 60:32:8, 1.6′\], \[10003L, 57, M, 58:36:6, 1.7′\], \[20205R, 44, M, 62:30:8, 1.6′\], \[20236R, 42, M, 61:30:9, 1.8′\], and \[20253R, 30, F, 62:30:8, 1.5′\]. All subjects self-reported as having normal color vision and no ocular disease condition. Subjects 10001R, 10003L, and 20205R are coauthors on the paper and were blinded to the test conditions but were aware of the purposes of the study. The other two subjects were members of the participating lab at the University of Washington but were naive to the purposes of the study. The studies were approved by the institutional review boards at the University of California, Berkeley (2020-02-12997) and the University of Washington (STUDY00013473). We obtained informed consent from all participants.

### Theoretical modeling of Oz color gamuts

The “Theory of cell-by-cell color” section presents a model of achievable Oz color gamut as a function of the fraction of light leaking into neighboring cells rather than the target cell. The model predicts the perceived LMS value given an input microdose wavelength, target LMS value, and subject cone ratio. We work in the 3D LMS coordinates defined by projection of spectral power distribution functions against the Stockman and Sharpe human cone responses ([*2*](#core-R2), [*3*](#core-R3)). We plot colors on the *lms* chromaticity triangle, with barycentric coordinates (l,m,s)\=(L,M,S)L+M+S.

Fractional leak is assumed to collect in neighboring L, M, and S cones in proportion to their relative frequency in the subject’s retina (reported per subject in the “Human subjects” section, and [Fig. 2Opens in image viewer](#F2) is for subject 10001R). For example, if the fractional leak is 60%, the model deposits 40% of the light from each microdose into the target cone and distributes the remaining 60% of the light uniformly into all other cones. The activation at each cell is a product of the total light received and the sensitivity of that cone to the light’s wavelength. The model’s predicted LMS value is computed as the average L, M, and S cone cell activation across the population of cells.

Idealized, diffraction-limited performance is shown in [Fig. 2AOpens in image viewer](#F2), and varies with retinal eccentricity because of increasing spacing of cones ([*36*](#core-R36)). The fractional leak is computed by assuming that each microdose is perfectly centered on its target cone cell, that adaptive optics achieves a diffraction-limited optical point-spread-function with a dilated 6-mm pupil, modeling a hexagonal packing of cone cells, and modeling the spatial light gathering function of each cone cell as equal to a Gaussian with a full-width-half-maximum that is half the cone inner segment diameter (ISD) ([*22*](#core-R22)). The 4° and foveola regions are modeled with center-to-center spacings of 1.6′ and 0.4′, respectively, consistent with Curcio *et al.* ([*36*](#core-R36)) (using the observed per-subject center-to-center spacings reported in the “Human subjects” section yields no substantial difference on predicted idealized performance). The ISD is assumed to be a fraction of the spacing: two-thirds at 4° and equality in the tightly packed foveola.

In [Fig. 2AOpens in image viewer](#F2), we also highlight the fractional leak that best fits the empirical color matching data. We compute a least-squares fit of the fractional leak parameter, to best explain the shift between target LMS values and matched LMS values in the experimental color matching data in the “Color matching experiments” section. It is important to compute the fit in the linear space of 3D LMS values, not in the projected 2D chromaticity space. In summary, this fit minimizes the root mean squared error (RMSE) in Cartesian LMS coordinates between the experimental data and our model’s output. The fit works as follows. For each experimental color match datum, we take the LMS target value and use the model to compute a predicted LMS value under a given fractional light leak. We compare the difference (modeling error) between this predicted LMS value and the subject’s experimental color match LMS value for that datum. We sum the errors for all the color match data in an RMSE sense. Then, we use a least squares solver to compute the fractional light leak value that minimizes the total modeling error. One practical detail is that there is a perceptual scaling factor between the predicted LMS value and the experimental LMS value, which differs for each experimental session, stimulation laser and cone class. These scaling factors are not known a priori but are inherent in the experimental match data; they represent unmeasured normalization of laser power across sessions and variation in individual subject cone response functions at the target laser powers. In the least squares solver, these scaling factors are variables (19 total), and we solve for the scaling factors and global fractional light leak that jointly minimize the total modeling error over all non-control color matches (190 total).

### Prototype system hardware

We built on an AOSLO platform described in previous publications ([*14*](#core-R14), [*37*](#core-R37)). In this study, we used four spectral channels: a 940-nm channel for wavefront sensing, an 840-nm channel for retinal imaging, a 543-nm channel for retinal stimulation, and a blue channel configurable either as a 488-nm channel for retinal stimulation, or as a wavelength-tunable monochromatic source for matching use. Laser sources of the 940-, 840-, and 543-nm channels are drawn from the broadband spectral output of a supercontinuum laser (EXR-15, NKT, Birkerød, Denmark). The laser source of the blue channel comes from a separate supercontinuum laser (FIU-15, NKT, Birkerød, Denmark) passed through a tunable filter (VARIA, NKT, Birkerød, Denmark).

All channels (except the 940-nm channel) are passed through individually fiber-coupled acousto-optic modulators (AOMs) (Brimrose Corporation, Sparks, MD) that can modulate laser intensity up to 50 MHz, and are coaligned to make the pupil-conjugate planes optically coincident for each channel pair, with respective beam vergence precompensated to be opposite to the longitudinal chromatic aberration of a typical human eye ([*38*](#core-R38)). These adjustments ensure that all wavelengths are focused approximately at the same axial retinal depth. At the eye station, the laser powers in all channels are eye-safe, and are measured and recorded before every session.

In experiments, a custom-built Shack-Hartmann wavefront sensor operating with 940-nm light enables adaptive optics correction of eye aberrations in real time using a deformable mirror (DM97, ALPAO, Montbonnot-Saint-Martin, France), to achieve near-diffraction-limited focus at the photoreceptor layer of the retina, for all wavelength channels. We dilate and cyloplege our subjects using eye drops (1% tropicamide and 2.5% phenylephrine) to enable imaging through the largest possible pupil size (highest numerical aperture) and use adaptive optics to measure and correct for the aberrations of the eye and strive for near-diffraction-limited performance ([*39*](#core-R39)).

This laser spot is scanned in a raster pattern over a 0.9° square field of view using orthogonally oriented resonant and galvo mirrors, with a frame resolution of 512 × 256 pixels and a frame rate of 60 Hz. Light scattered from the retina is descanned and spectrally redirected to confocal pinholes mounted to individual photomultiplier tubes (PMTs) for each wavelength channel.

A custom-built field-programmable gate array (FPGA) board \[initially in ([*40*](#core-R40))\] digitizes and aggregates all PMT signals into 512 × 16 pixel strips of each frame at 960 Hz as a rolling shutter video stream into a graphics processing unit (GPU) desktop computer that computes cone-by-cone microdose targets (see the “Prototype system software” section). The FPGA receives rasterized 14-bit stimulation signals from the desktop to drive AOMs that modulate the visible-wavelength laser intensities and deliver the intended microdoses of laser light to real-time cone positions.

In addition, a separate optically coaligned pathway, similar to the one used in ([*41*](#core-R41)), is used, which incorporates a projector display for showing fixation points and color matching targets, and a pupil camera for real-time pupil tracking.

### Prototype system software

#### *Creating the spectrally classified retina map*

We create a 1.8° square retina map for the subject ([Fig. 1AOpens in image viewer](#F1)) comprising a composite infrared image of the retina with metadata for the location of each cone and its spectral type. Although cone positions and types are stable, the reflectance appearance of different cones will change over time. We construct the composite retina image by first acquiring a set of 3 × 3 infrared AOSLO videos of the subject’s retina, with fields of view that overlap by 50% each to cover the desired area. We extend a global optimization algorithm, R-SLAM ([*42*](#core-R42)), to jointly solve for the composite retina image from the overlapping videos, as well as distortion corrections in the sinusoidal velocity of the resonant mirror. We use the Retina Map Alignment Tool \[described in ([*26*](#core-R26))\], to align and copy over metadata about each cone’s location and spectral type from a master retina map for the subject. This master map of spectral classification is created once per subject through optoretinography on a separate AO-OCT instrument ([*23*](#core-R23)).

#### *Eye motion tracking*

We computationally track eye gaze translation and torsion with subcellular accuracy, by comparing the incoming 960-Hz stream of video strips against the retina map. We build on the normalized cross-correlation (NCC) strip-based matching method ([*43*](#core-R43)). We reduce false NCC matches using RANSAC ([*44*](#core-R44)), with full implementation detailed in ([*45*](#core-R45)). We add measurement of torsion by separately tracking the left and right halves of each incoming strip, fitting a matrix of rotation and translation to the midpoints of the independently tracked halves ([*43*](#core-R43)). This tracking algorithm runs on average in 0.8 ms on a GPU (GeForce RTX 3090, NVIDIA, Santa Clara, CA).

#### *Computing target cone activations and laser microdose intensities*

As shown in [Fig. 1Opens in image viewer](#F1), we render the target video into a dynamic stream of target cone activations and laser microdose intensities. First, in a preprocessing step, we convert the RGB video pixels into LMS colorspace and spatially downsample to 64 × 64 pixel resolution to approximately match the spatial resolution of the cone mosaic within the system field of view.

During Oz stimulation, we render target video pixels into target cone activations. We store the cone target activations as a piece of metadata for each cone cell in the retina map. At the frame rate of the video, we continuously increment the target activation of all cones that fall within the bounds of the video at that instant, according to the real-time eye motion tracking. The activation increment for each cone is equal to the LMS video pixel value at the real-time location of the cone, taking the value in the LMS color channel that matches the spectral type of the cone.

In parallel computation, the desktop system rasterizes cone target activations into laser raster pixel values. Rasterization clears cone target activations by zeroing them out after instructing the FPGA to illuminate the laser raster pixel that will send a physical microdose of light to the cone’s real-time position on the retina. Rasterization is implemented by just-in-time computation and transmission to the FPGA of the 512 × 16 strip of raster pixel values that it will then stream to the laser modulation units. The real-time location of the current strip is defined by eye motion tracking and the offset of the strip within the video frame. For each cone on the retina map that is contained within the real-time location of the current strip, the cone’s position within the pixel array is located, and the corresponding pixel is set to the desired laser microdose intensity. This intensity value is equal to the target activation multiplied by the relative power of the laser, divided by the spectral response of that type of cone at the wavelength of the laser. During rasterization, microdoses can be programmably micro-adjusted; for example, the jitter control condition is implemented by randomly displacing the microdose pixel from the true location of the target cone in the pixel array. In 3 of 20 color matching sessions, the jitter control is implemented by splitting each microdose into four, displaced to the corners of a square at two cone spacings away from the target cone. Rasterization and FPGA data transmission also comprehensively account for the system’s spatiotemporal properties, including: computational latency of eye tracking and data transmission, sinusoidal scanning velocity of the resonant mirrors, and timing offsets in individual PMT outputs and AOM inputs relative to a common clock.

#### *Transverse chromatic aberration compensation*

Transverse chromatic aberration (TCA) causes the different wavelength channels of the laser beam to refract to different lateral locations on the retina. The effect is that the pixel rasters for each laser wavelength differ in offset and magnification, often by several cones. Measuring TCA translation and magnification enables simple geometric compensation in the rasterization step of the previous section. We improve on previous image-based procedures ([*46*](#core-R46)) for measuring TCA, which are based on simultaneous reflection imaging of all wavelength channels by interlacing lines for each channel in the raster stream. The NCC algorithm is used to spatially align each of the resulting videos, with the required translation being the required TCA measurement.

The key challenge that we improve upon is high noise in video signals for visible channels due to low reflected photon counts at the 543 and 488 nm PMTs. We find the noise is too high in 488 for successful NCC-based alignment. Fortunately, the infrared video usually has excellent signal-to-noise ratio. Our improvement is to modify the R-SLAM algorithm ([*42*](#core-R42)) to process the interlaced video recordings, using the infrared image to spatially stabilize and remove motion distortions from all interlaced channels including the noisy ones, and generating low-noise retinal maps for all wavelength channels by temporal averaging of the stabilized and undistorted video frames. We further extend the prior work to solve for both translation and scaling components of TCA between the different wavelength channels.

We also computationally verify the correctness of TCA measurement and compensation before beginning experiments. We draw a fiducial mark in each interlaced channel, by zeroing out the pixels for the fiducial in all channels using TCA compensation. The resulting retina videos for each channel will show the fiducial as black pixels in the retina imagery; correct TCA is verified by spatially aligning the cone imagery in the different channels and confirming that the fiducial marks in all channels are simultaneously aligned. We use the R-SLAM method to generate low-noise retina images in all channels, as described, and by drawing a grid of fiducials to visually verify TCA compensation across the entire field of view.

TCA is highly dependent on pupil location ([*46*](#core-R46)). We position the subject’s head precisely via a bitebar and use a pupil tracker and manual bitebar position adjustments to maintain the same pupil position during experiments as during TCA measurement.

### Color matching experiments

#### *Color matching tools*

The RGB projector that we use for color matching is a DLP LightCrafter projector (Wintech, Carlsbad, CA), which is coaligned with the AOSLO’s optical path. We control this display using the Psychophysics Toolbox ([*47*](#core-R47)–[*49*](#core-R49)). The tunable-laser source that we use for color matching is a SuperK supercontinuum laser connected to the SuperK VARIA tunable filter (NKT Photonics, Birkerød, Denmark) that allows interactive selection of the spectral bandwidth and center wavelength.

Our custom color matching control panel is a Behringer X-Touch MINI USB Controller (Behringer, Willich, Germany), which we programmed specifically for the color matching task. In experiments using the RGB projector, subjects use three free-wheeling dials on this controller to adjust the hue, saturation, and value of the projector-generated matching square and an additional dial to add desaturating white light to the Oz square if necessary to achieve an overall match. In experiments using the tunable-wavelength source, subjects use two dials to adjust the center wavelength and overall intensity of the matching square, and have the option of using two additional dials to add desaturating white light either to the stimulus or the matching field. We use a bandwidth of 10 nm, and the center wavelength can be adjusted over a range from 405 to 525 nm.

#### *Subject view*

A sketch of the subject’s view during color matching is shown in [Fig. 5Opens in image viewer](#F5). The projector displays a gaze target that we instruct subjects to fixate on for the duration of the experiment. Approximately 4° adjacent to the gaze target, there is a 0.9° square in which both the Oz color and matching color appear in alternation. This geometry ensures that the AOSLO stimulates the classified region of the subject’s retina.

[![](/cms/10.1126/sciadv.adu1052/asset/c22483c4-b38d-46d4-a57a-69f6ce368513/assets/images/large/sciadv.adu1052-f5.jpg)](#F5)

Fig. 5. Subject’s view during color matching experiment.

Left shows the experimental view. Right shows an example of the multicolored mosaics shown for a periodic 15-s “refresh period.”

[Open in viewer](#F5)

The projector generates a gray background field that is 17° in diameter. The chromaticity of this gray background is taken as the reference white point for plotting in [Fig. 3Opens in image viewer](#F3). The luminance of this gray background is kept at approximately 500 cd/m2 to establish photopic light levels and avoid interference from rod activation. The gray background pixels are turned off within the Oz/matching square so that no projector light is added to either stimulus. This black square is 1.2× the size of the stimulus, so that any slight misalignment between the AOSLO and projector display does not cause the stimuli to mix with the gray background.

#### *Color matching protocol*

During a color matching trial, subjects adjust the controllable color until they achieve a match with the Oz color. The Oz and matching squares of color are spatially coincident and alternate in time, turning on for 1 s each followed by 1 s of darkness in a repeating cycle. Subjects are free to take as much time as necessary to achieve a match. Every 30 s, there is a “refresh period” during which the entire field is replaced with a series of multicolored mosaics inspired by the “wipeout” pattern from prior work in hue matching under fixation ([*11*](#core-R11)). Each mosaic appears for 1 s, and each refresh period lasts 15 s. This period is intended to mitigate the effect of afterimages, which diminish the apparent color saturation of the matching field and Oz stimulus. To submit a match, subjects must have undergone this refresh period within 12 s prior and not made any further color adjustments. This is to ensure that the submitted match can be quickly confirmed while the presence of afterimages is minimal.

As an option, subjects can view the Oz and matching squares vertically side by side, synchronously alternating 2-s on and 2-s off. This allows simultaneous comparison of the two colors, but subjects must return to the spatially overlapped and temporally alternating view to judge the match using a common patch of adapted retina before submission.

Subjects may note that the color and luminance of the stimulus is not uniform across the entire field of stimulation. In particular, some parts of the field may occasionally appear as the color of the stimulating laser itself, regardless of the LMS activation levels being targeted. This is because any errors in delivery will tend to cause the percept to skew toward the color of the stimulating laser. Subjects are told to match to the color they see that is most dissimilar from the natural color of the laser.

#### *Plotting color matches*

To compute LMS color match coordinates, we spectrally characterize the RGB projector and the tunable laser using a spectroradiometer \[PR-650, PhotoResearch (now Jadak), North Syracuse, NY\]. We measure the spectral power distribution for the projector’s red, green, and blue pixel primaries and the linearity of each primary across its 8-bit range, subject to a lookup table, which we compute in a calibration step. We compute color match coordinates as the weighted sum of the red, green, and blue primary spectra, each scaled according to the measured output levels corresponding to the matching RGB values selected by the subject. For matches made using the tunable laser system, we individually measure the spectrum of each match setting that was submitted during the experiment. Any match involving both the projector’s desaturating white and the tunable-wavelength source can be constructed by combining the spectra for the two sources. All color match average points are computed in 3D LMS coordinates before projection into *lms* chromaticity space.

The projector’s gamut is the triangle enclosed by the chromaticity coordinates of its red, green, and blue primaries, as shown in [Fig. 3 (A and B)Opens in image viewer](#F3). The tunable laser’s gamut is the edge of the spectral locus corresponding to wavelengths between 405 and 525 nm. When combined with white light from the projector, its overall gamut for matching is encompassed by the region outlined in [Fig. 3COpens in image viewer](#F3). To match colors beyond the boundary of these gamuts, the subject adds an amount of projector white to desaturate the Oz color until a color match is achievable. The resulting coordinates for the Oz color are the coordinates of this desaturating white subtracted from the coordinates of the matching color.

In [Figs. 2Opens in image viewer](#F2), [3Opens in image viewer](#F3), and [6Opens in image viewer](#F6), we draw the natural human gamut as reference, using colors that approximately convey how those chromaticities would appear to test subjects, by mapping (*l*, *m*, *s*) to sRGB ([*50*](#core-R50)) colors with matching CIELAB hue ([*51*](#core-R51)).

[![](/cms/10.1126/sciadv.adu1052/asset/fa2481cf-1382-40a7-b61c-0188477622b2/assets/images/large/sciadv.adu1052-f6.jpg)](#F6)

Fig. 6. Computation of perceptual uncertainty ellipses.

(**A**) Human perceptual JND ellipsoids \[e.g., *E*(*c*) at color *c*\] are long and skinny, pointing at the origin in LMS Cartesian space. They project to ellipses in the (l,m,s)\=(L,M,S)L+M+S chromaticity triangle as shown. (**B**) Vector math for computing the coordinates of a color match *c**m* = *c**p* − *c**n*, where *c**p* is the “positive” color seen by subject at matching, and *c**n* is the “negative” light added to the test color to enable a match. (**C**) In color matching, because the color seen by the subject at the time of match submission is the “positive” color *c**p*, the perceptual uncertainty of the inferred color match *c**m* is the ellipsoid *E*(*c**p*), recentered on *c**m*, as shown. If *c**n* is non-negative, as shown, the ellipsoid recentered on *c**m*, *E*(*c**p*) − *c**n*, no longer points at the origin, and projects to a nonlinearly enlarged ellipse in the *lms* triangle. Therefore, it is desirable to minimize the “negative” light required to achieve a color match, as accomplished with the tunable monochromatic light source used for matching olo (see [Fig. 3Opens in image viewer](#F3)).

[Open in viewer](#F6)

Care must be exercised in comparing the relative positions of color match points and the boundary of the human gamut. Plotting both of these relies on standard Stockman-Sharpe cone spectral response functions. The exact cone response functions for subjects are unknown, but in general, individual cone responses will vary from the standard, for example, due to photopigment optical density. Using variations in the assumed cone responses during plotting will cause small geometric shifts in the spectral locus and color match points, and it is possible for color match points in [Fig. 3 (A and B)Opens in image viewer](#F3) near the boundary to fall outside or inside the gamut depending on the cone responses assumed. In contrast, note that the color match points in [Fig. 3COpens in image viewer](#F3) always fall outside the human gamut, because subjects compare olo against near-monochromatic colors that lie on the boundary of the gamut regardless of the cone responses assumed in plotting.

#### *Plotting perceptual uncertainty in matching*

In [Fig. 3Opens in image viewer](#F3), we plot the relative perceptual uncertainty of color matches as ellipses around the color coordinates. These ellipses are proportional in size to the perceptually just-noticeable-difference (JND) region of colors surrounding the matching color. Notably, these perceptual ellipses in [Fig. 3Opens in image viewer](#F3) are small for orange matches and relatively large for colors that are outside the color matching system’s gamut, like olo. It turns out that this phenomenon is entirely due to two factors that we explain in this section: the natural shape of human perceptual uncertainty ellipsoids in 3D colorspace and the vector geometry of calculating chromaticities from color matching results. [Figure 6Opens in image viewer](#F6) illustrates the phenomenon.

First, human color discrimination ability varies across colorspace as first quantified by MacAdam ([*52*](#core-R52)) and codified in standards such as CIELAB and ΔEab\* ([*51*](#core-R51)). We use the CIELAB standard to estimate the relative size of JND spheres \[ΔEab\*\=2.3 ([*53*](#core-R53))\], then transform them into LMS colorspace, where they become ellipsoids. It is a feature of human perception that these ellipsoids are long and skinny, pointing toward the origin of LMS space as shown in [Fig. 6AOpens in image viewer](#F6). Since plotting *lms* chromaticity coordinates involves projection through the origin \[(l,m,s)\=(L,M,S)L+M+S\], these ellipsoids project to relatively small 2D error ellipses as shown on the triangle in [Fig. 6AOpens in image viewer](#F6). Let us denote the JND ellipsoid at a given color *c**o* in LMS coordinates as *E*(*c**o*).

Second, the vector geometry of calculating color matches is shown in [Fig. 6BOpens in image viewer](#F6). Let us denote the inferred LMS color coordinates of the test color as *c**m* = *c**p* − *c**n*, where *c**p* represents the LMS coordinates of the “positive” color actually seen by the subject when submitting the color match result, and *c**n* represents the LMS coordinates of the “negative” light added by the subject to the test color. A critical observation is that the perceptual uncertainty of a color match is defined by human color discrimination power around *c**p*, the “positive” color seen at submission, not the test color *c**m*. To prove this, note that when the color match is submitted, the human subject is looking at colors *c**p* and (*c**m* + *c**n*) in alternation, and judging them to be perceptually a match. Any color within *E*(*c**p*), the perceptual JND ellipsoid at *c**p*, would have been perceptually indistinguishable and also considered a match at submission. Therefore, the perceptual uncertainty ellipsoid for a color matching result is *E*(*c**p*) centered at *c**m* = *c**p* − *c**n*, as shown in [Fig. 6COpens in image viewer](#F6).

Last, when *c**n* is nonzero, the vector shift of the long, skinny ellipsoid means that it is no longer oriented toward the origin in LMS colorspace. Therefore, projecting toward the origin to compute the *lms* chromaticity will cause the long axis of the ellipsoid to spread into a larger ellipse on the chromaticity diagram. This effect grows nonlinearly as *c**n* increases, as shown in [Fig. 6COpens in image viewer](#F6).

To recap, the ellipses drawn in [Fig. 3Opens in image viewer](#F3) are simply the relative perceptual uncertainty around the color match coordinate computed by the process above, drawn three times the actual size of a projected JND ellipsoid computed by the CIELAB/Δ*E* standard. The shape of these ellipses depends only on the position of the subject’s average match location and are not derived from the spread of the matches themselves. That is, they account for potential noise in matching due to the perceptual characteristics of color space near the match, but do not reflect other sources of variation, such as individual observer differences or system performance variations from trial to trial.

Note that CIELAB is defined with respect to an assumed white point. In our calculations, the white point has the chromaticity matching the gray background field ([Fig. 5Opens in image viewer](#F5)) and the CIE Y luminance of either the stimulus or the background, whichever is greater.

#### *Qualitative hue naming and saturation rating*

In addition to color matching, we carry out color naming experiments in which subjects observe a stimulus, then name its hue and rate its saturation on a scale from 1 to 4. Subjects see the same fixation cross and gray background, with stimuli appearing at 4° eccentricity from fixation, as in the color matching experiments. Stimuli turn on and off for 2 s each on a continuous cycle, and a refresh period occurs before each new stimulus is shown. Subjects are instructed to observe the stimulus for as long as necessary, then to report their qualitative description.

### Image and video recognition experiments

In the 4-AFC and 2-AFC tasks, we aim to eliminate any luminance cues that could allow subjects to detect line orientation or motion direction without using hue information. In particular, because our subjects have more L cones than M cones, a higher density of microdoses are directed at the foreground object than the background in AFC stimuli, which would create a difference in luminance between them. Thus, in a pre-experiment calibration step, subjects adjust the intensity of microdoses directed at the L cones while viewing a jittered moving disk stimulus until the field appears equiluminant. This selected intensity level is then used to render any L-targeted microdoses in the subsequent experiments.

## Acknowledgments

We thank A. Kotani for graphics used in movie S1; B. Wendel for assistance in cone classification; R. Weber from Montana State University for FPGA programming; J. Shenoy for implementation of R-SLAM; R. Upadhyay for implementation of the Retina Map Alignment Tool; A. Aikawa, E. Alexander, H. Johnson, L. Y. Kat, J. Ku, P. Manohar, V. Ramakrishnan, A. Sabnis, U. Singhal, S. Sun, J. Tan, J. Zhang, and Y. Zong for assistance in preliminary system engineering; A. Belsten, D. Brainard, N. Jennings, A. Kotani, J. Lee, B. Olshausen, F. Rieke, V. N. Srivastava, A. Thakrar, and B. Wandell for reading and commenting on this manuscript; and P. Bharadwaj and S. Schleufer for additional support. C.W., A.E.B., S.R.H., B.P.S., and J.E.V. completed this work entirely at UC Berkeley, and are now, respectively, affiliated with Princeton University, Google Inc., Rochester Institute of Technology, Etsy Inc., and Exponent Inc.

**Funding:** This work was supported by a Hellman Fellowship (R.N.), FHL Vive Center Seed Grant (R.N.), Air Force Office of Scientific Research grant FA9550-20-1-0195 (J.F., C.W., W.S.T., A.R., and R.N.), Air Force Office of Scientific Research grant FA9550-21-1-0230 (J.F., H.K.D., C.W., A.E.B., S.R.H., V.P.P., W.S.T., R.S., A.R., and R.N.), National Institutes of Health grant R01EY023591 (A.E.B., B.P.S., P.T., J.E.V., W.S.T., A.R., and R.N.), National Institutes of Health grant R01EY029710 (V.P.P. and R.S.), National Institutes of Health grant U01EY032055 (V.P.P., R.S., and A.R.), and a Burroughs Wellcome Fund Career Award at the Scientific Interface (R.S.).

**Author contributions:** Conceptualization: J.F., H.K.D., A.E.B., V.P.P., B.P.S., A.R., and R.N. Methodology: J.F., H.K.D., C.W., A.E.B., V.P.P., B.P.S., R.S., A.R., and R.N. Investigation: J.F., H.K.D., C.W., A.E.B., S.R.H., V.P.P., B.P.S., J.E.V., W.S.T., R.S., A.R., and R.N. Formal analysis: J.F., H.K.D., A.E.B., V.P.P., R.S., and R.N. Resources: V.P.P., P.T., W.S.T., R.S., A.R., and R.N. Funding acquisition: W.S.T., R.S., A.R., and R.N. Project administration: J.F., H.K.D., R.S., A.R., and R.N. Supervision: W.S.T., R.S., A.R., and R.N. Visualization: J.F., H.K.D., A.R., and R.N. Data curation: J.F., H.K.D., C.W., V.P.P., A.R., and R.N. Validation: J.F., H.K.D., C.W., V.P.P., A.R., and R.N. Software: J.F., H.K.D., C.W., A.E.B., S.R.H., V.P.P., B.P.S., P.T., and R.N. Writing–original draft: J.F., H.K.D., C.W., and R.N. Writing–review and editing: J.F., H.K.D., A.E.B., S.R.H., V.P.P., B.P.S., J.E.V., W.S.T., A.R., and R.N.

**Competing interests:** The Regents of the University of California has filed a patent for cell-by-cell retina stimulation, for which R.N., A.R., and B.P.S. are inventors (WO2020086612A1). V.P.P. and R.S. have filed a US patent application describing the technology for the linescan OCT for optoretinography. All other authors declare that they have no competing interests.

**Data and materials availability:** Code and data underlying the study are deposited in Dryad at [https://doi.org/10.5061/dryad.pc866t206](https://doi.org/10.5061/dryad.pc866t206). All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials.

## Supplementary Materials

### The PDF file includes:

Legend for movie S1

*   [Download](/doi/suppl/10.1126/sciadv.adu1052/suppl_file/sciadv.adu1052_sm.pdf)
*   193.37 KB

### Other Supplementary Material for this manuscript includes the following:

Movie S1

*   [Download](/doi/suppl/10.1126/sciadv.adu1052/suppl_file/sciadv.adu1052_movie_s1.zip)
*   44.30 MB

## REFERENCES AND NOTES

1

M. Fairchild, “Colorimetry” in *Color Appearance Models*, (John Wiley & Sons, 2013), pp. 56–84.

[Google Scholar](https://scholar.google.com/scholar?q=M.+Fairchild%2C+%E2%80%9CColorimetry%E2%80%9D+in+Color+Appearance+Models%2C+%28John+Wiley+%26+Sons%2C+2013%29%2C+pp.+56%E2%80%9384.)

*   [a \[...\] bounded color gamut of natural human vision](#body-ref-R1-1)
*   [b \[...\] the retina. As usual with color matching](#body-ref-R1-2)

2

A. Stockman, L. T. Sharpe, C. Fach, The spectral sensitivity of the human short-wavelength sensitive cones derived from thresholds and color matches. *Vision Res.* **39**, 2901–2927 (1999).

[Crossref](https://doi.org/10.1016/S0042-6989\(98\)00225-9)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10492818/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+spectral+sensitivity+of+the+human+short-wavelength+sensitive+cones+derived+from+thresholds+and+color+matches&author=A.+Stockman&author=L.+T.+Sharpe&author=C.+Fach&publication_year=1999&journal=Vision+Res.&pages=2901-2927&doi=10.1016%2FS0042-6989%2898%2900225-9&pmid=10492818)

*   [a \[...\] S cones and overlaps completely with them](#body-ref-R2-1)
*   [b \[...\] Stockman and Sharpe human cone responses](#body-ref-R2-2)

3

A. Stockman, L. T. Sharpe, The spectral sensitivities of the middle- and long-wavelength-sensitive cones derived from measurements in observers of known genotype. *Vision Res.* **40**, 1711–1737 (2000).

[Crossref](https://doi.org/10.1016/S0042-6989\(00\)00021-3)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10814758/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+spectral+sensitivities+of+the+middle-+and+long-wavelength-sensitive+cones+derived+from+measurements+in+observers+of+known+genotype&author=A.+Stockman&author=L.+T.+Sharpe&publication_year=2000&journal=Vision+Res.&pages=1711-1737&doi=10.1016%2FS0042-6989%2800%2900021-3&pmid=10814758)

*   [a \[...\] S cones and overlaps completely with them](#body-ref-R3-1)
*   [b \[...\] Stockman and Sharpe human cone responses](#body-ref-R3-2)

4

R. Sabesan, B. P. Schmidt, W. S. Tuten, A. Roorda, The elementary representation of spatial and color vision in the human retina. *Sci. Adv.* **2**, e1600797 (2016).

[Crossref](/servlet/linkout?suffix=e_1_3_2_5_2&dbid=4&doi=10.1126%2Fsciadv.adu1052&key=10.1126%2Fsciadv.1600797&site=aaas-site)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/27652339/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000383734400014)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+elementary+representation+of+spatial+and+color+vision+in+the+human+retina&author=R.+Sabesan&author=B.+P.+Schmidt&author=W.+S.+Tuten&author=A.+Roorda&publication_year=2016&journal=Sci.+Adv.&pages=e1600797&doi=10.1126%2Fsciadv.1600797&pmid=27652339)

*   [a \[...\] M cones is targeting light to only one](#body-ref-R4-1)
*   [b \[...\] that builds upon the cone-targeted methods](#body-ref-R4-2)
*   [c \[...\] to contribute to a stable color percept](#body-ref-R4-3)

5

H. Hofer, B. Singer, D. R. Williams, Different sensations from cones with the same photopigment. *J. Vis.* **5**, 444–454 (2005).

[GO TO REFERENCE](#body-ref-R5)

[Crossref](https://doi.org/10.1167/5.5.5)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/16097875/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000229664000005)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Different+sensations+from+cones+with+the+same+photopigment&author=H.+Hofer&author=B.+Singer&author=D.+R.+Williams&publication_year=2005&journal=J.+Vis.&pages=444-454&doi=10.1167%2F5.5.5&pmid=16097875)

6

B. P. Schmidt, A. E. Boehm, K. G. Foote, A. Roorda, The spectral identity of foveal cones is preserved in hue perception. *J. Vis.* **18**, 19 (2018).

[Crossref](https://doi.org/10.1167/18.11.19)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/30372729/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000451282200019)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+spectral+identity+of+foveal+cones+is+preserved+in+hue+perception&author=B.+P.+Schmidt&author=A.+E.+Boehm&author=K.+G.+Foote&author=A.+Roorda&publication_year=2018&journal=J.+Vis.&pages=19&doi=10.1167%2F18.11.19&pmid=30372729)

7

B. P. Schmidt, R. Sabesan, W. S. Tuten, J. Neitz, A. Roorda, Sensations from a single M-cone depend on the activity of surrounding S-cones. *Sci. Rep.* **8**, 8561 (2018).

[Crossref](https://doi.org/10.1038/s41598-018-26754-1)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/29867090/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000434011100048)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Sensations+from+a+single+M-cone+depend+on+the+activity+of+surrounding+S-cones&author=B.+P.+Schmidt&author=R.+Sabesan&author=W.+S.+Tuten&author=J.+Neitz&author=A.+Roorda&publication_year=2018&journal=Sci.+Rep.&pages=8561&doi=10.1038%2Fs41598-018-26754-1&pmid=29867090)

*   [a \[...\] cone cells preclassified by spectral type](#body-ref-R7-1)
*   [b \[...\] M cones is targeting light to only one](#body-ref-R7-2)
*   [c \[...\] to contribute to a stable color percept](#body-ref-R7-3)

8

B. P. Schmidt, A. E. Boehm, W. S. Tuten, A. Roorda, Spatial summation of individual cones in human color vision. *PLOS ONE* **14**, e0211397 (2019).

[Crossref](https://doi.org/10.1371/journal.pone.0211397)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/31344029/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000484977900002)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Spatial+summation+of+individual+cones+in+human+color+vision&author=B.+P.+Schmidt&author=A.+E.+Boehm&author=W.+S.+Tuten&author=A.+Roorda&publication_year=2019&journal=PLOS+ONE&pages=e0211397&doi=10.1371%2Fjournal.pone.0211397&pmid=31344029)

*   [a \[...\] ) or two](#body-ref-R8-1)
*   [b \[...\] that builds upon the cone-targeted methods](#body-ref-R8-2)

9

G. S. Brindley, The effects on colour vision of adaptation to very bright lights. *J. Physiol.* **122**, 332–350 (1953).

[Crossref](https://doi.org/10.1113/jphysiol.1953.sp005003)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/13118543/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+effects+on+colour+vision+of+adaptation+to+very+bright+lights&author=G.+S.+Brindley&publication_year=1953&journal=J.+Physiol.&pages=332-350&doi=10.1113%2Fjphysiol.1953.sp005003&pmid=13118543)

*   [a \[...\] red light before displaying green light](#body-ref-R9-1)
*   [b \[...\] so they are difficult to measure precisely](#body-ref-R9-2)

10

P. Churchland, Chimerical colors: Some phenomenological predictions from cognitive neuroscience. *Philos. Psychol.* **18**, 527–560 (2005).

[GO TO REFERENCE](#body-ref-R10)

[Crossref](https://doi.org/10.1080/09515080500264115)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Chimerical+colors%3A+Some+phenomenological+predictions+from+cognitive+neuroscience&author=P.+Churchland&publication_year=2005&journal=Philos.+Psychol.&pages=527-560&doi=10.1080%2F09515080500264115)

11

J. Koenderink, A. van Doorn, C. Witzel, K. Gegenfurtner, Hues of color afterimages. *i-Perception* **11**, 1–18 (2020).

[Crossref](https://doi.org/10.1177/2041669520903553)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Hues+of+color+afterimages&author=J.+Koenderink&author=A.+van+Doorn&author=C.+Witzel&author=K.+Gegenfurtner&publication_year=2020&journal=i-Perception&pages=1-18&doi=10.1177%2F2041669520903553)

*   [a \[...\] so they are difficult to measure precisely](#body-ref-R11-1)
*   [b \[...\] prior work in hue matching under fixation](#body-ref-R11-2)

12

O. Estévez, H. Spekreijse, The “silent substitution” method in visual research. *Vision Res.* **22**, 681–691 (1982).

[GO TO REFERENCE](#body-ref-R12)

[Crossref](https://doi.org/10.1016/0042-6989\(82\)90104-3)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/7112962/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+%E2%80%9Csilent+substitution%E2%80%9D+method+in+visual+research&author=O.+Est%C3%A9vez&author=H.+Spekreijse&publication_year=1982&journal=Vision+Res.&pages=681-691&doi=10.1016%2F0042-6989%2882%2990104-3&pmid=7112962)

13

R. L. De Valois, K. K. De Valois, E. Switkes, L. Mahon, Hue scaling of isoluminant and cone-specific lights. *Vision Res.* **37**, 885–897 (1997).

[GO TO REFERENCE](#body-ref-R13)

[Crossref](https://doi.org/10.1016/S0042-6989\(96\)00234-9)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/9156186/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1997WN64500007)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Hue+scaling+of+isoluminant+and+cone-specific+lights&author=R.+L.+De+Valois&author=K.+K.+De+Valois&author=E.+Switkes&author=L.+Mahon&publication_year=1997&journal=Vision+Res.&pages=885-897&doi=10.1016%2FS0042-6989%2896%2900234-9&pmid=9156186)

14

A. Roorda, F. Romero-Borja, W. J. Donnelly III, H. Queener, T. J. Hebert, M. C. W. Campbell, Adaptive optics scanning laser ophthalmoscopy. *Opt. Express* **10**, 405–412 (2002).

[Crossref](https://doi.org/10.1364/OE.10.000405)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/19436374/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000175459200003)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Adaptive+optics+scanning+laser+ophthalmoscopy&author=A.+Roorda&author=F.+Romero-Borja&author=W.+J.+Donnelly&author=H.+Queener&author=T.+J.+Hebert&author=M.+C.+W.+Campbell&publication_year=2002&journal=Opt.+Express&pages=405-412&doi=10.1364%2FOE.10.000405&pmid=19436374)

*   [a \[...\] scanning light ophthalmoscopy (AOSLO)](#body-ref-R14-1)
*   [b \[...\] We build our Oz prototype on an AOSLO](#body-ref-R14-2)
*   [c \[...\] platform described in previous publications](#body-ref-R14-3)

15

F. Zhang, K. Kurokawa, A. Lassoued, J. A. Crowell, D. T. Miller, Cone photoreceptor classification in the living human eye from photostimulation-induced phase dynamics. *Proc. Natl. Acad. Sci. U.S.A.* **116**, 7951–7956 (2019).

[Crossref](https://doi.org/10.1073/pnas.1816360116)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/30944223/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000464767500055)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Cone+photoreceptor+classification+in+the+living+human+eye+from+photostimulation-induced+phase+dynamics&author=F.+Zhang&author=K.+Kurokawa&author=A.+Lassoued&author=J.+A.+Crowell&author=D.+T.+Miller&publication_year=2019&journal=Proc.+Natl.+Acad.+Sci.+U.S.A.&pages=7951-7956&doi=10.1073%2Fpnas.1816360116&pmid=30944223)

*   [a \[...\] optical coherence tomography (AO-OCT)](#body-ref-R15-1)
*   [b \[...\] techniques in an AO-OCT system](#body-ref-R15-2)

16

V. P. Pandiyan, X. Jiang, A. Maloney-Bertelli, J. A. Kuchenbecker, U. Sharma, R. Sabesan, High-speed adaptive optics line-scan OCT for cellular-resolution optoretinography. *Biomed. Opt. Express* **11**, 5274–5296 (2020).

[GO TO REFERENCE](#body-ref-R16)

[Crossref](https://doi.org/10.1364/BOE.399034)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/33014614/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000577455500030)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=High-speed+adaptive+optics+line-scan+OCT+for+cellular-resolution+optoretinography&author=V.+P.+Pandiyan&author=X.+Jiang&author=A.+Maloney-Bertelli&author=J.+A.+Kuchenbecker&author=U.+Sharma&author=R.+Sabesan&publication_year=2020&journal=Biomed.+Opt.+Express&pages=5274-5296&doi=10.1364%2FBOE.399034&pmid=33014614)

17

A. Roorda, D. R. Williams, The arrangement of the three cone classes in the living human eye. *Nature* **397**, 520–522 (1999).

[GO TO REFERENCE](#body-ref-R17)

[Crossref](https://doi.org/10.1038/17383)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10028967/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000078574900049)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+arrangement+of+the+three+cone+classes+in+the+living+human+eye&author=A.+Roorda&author=D.+R.+Williams&publication_year=1999&journal=Nature&pages=520-522&doi=10.1038%2F17383&pmid=10028967)

18

D. W. Arathorn, Q. Yang, C. R. Vogel, Y. Zhang, P. Tiruveedhula, A. Roorda, Retinally stabilized cone-targeted stimulus delivery. *Opt. Express* **15**, 13731–13744 (2007).

[GO TO REFERENCE](#body-ref-R18)

[Crossref](https://doi.org/10.1364/OE.15.013731)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/19550644/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000251223100029)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Retinally+stabilized+cone-targeted+stimulus+delivery&author=D.+W.+Arathorn&author=Q.+Yang&author=C.+R.+Vogel&author=Y.+Zhang&author=P.+Tiruveedhula&author=A.+Roorda&publication_year=2007&journal=Opt.+Express&pages=13731-13744&doi=10.1364%2FOE.15.013731&pmid=19550644)

19

J. C. Maxwell, Experiments on colour, as perceived by the eye, with remarks on colour-blindness. *Earth Environ. Sci. Trans. R. Soc. Edinb.* **21**, 275–298 (1857).

[GO TO REFERENCE](#body-ref-R19)

[Crossref](https://doi.org/10.1017/S0080456800032117)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Experiments+on+colour%2C+as+perceived+by+the+eye%2C+with+remarks+on+colour-blindness&author=J.+C.+Maxwell&publication_year=1857&journal=Earth+Environ.+Sci.+Trans.+R.+Soc.+Edinb.&pages=275-298&doi=10.1017%2FS0080456800032117)

20

D. I. Macleod, D. R. Williams, W. Makous, A visual nonlinearity fed by single cones. *Vision Res.* **32**, 347–363 (1992).

[GO TO REFERENCE](#body-ref-R20)

[Crossref](https://doi.org/10.1016/0042-6989\(92\)90144-8)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/1574850/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=A+visual+nonlinearity+fed+by+single+cones&author=D.+I.+Macleod&author=D.+R.+Williams&author=W.+Makous&publication_year=1992&journal=Vision+Res.&pages=347-363&doi=10.1016%2F0042-6989%2892%2990144-8&pmid=1574850)

21

N. Sekiguchi, D. R. Williams, D. H. Brainard, Efficiency in detection of isoluminant and isochromatic interference fringes. *J. Opt. Soc. Am. A* **10**, 2118–2133 (1993).

[Crossref](https://doi.org/10.1364/JOSAA.10.002118)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1993LZ15700002)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Efficiency+in+detection+of+isoluminant+and+isochromatic+interference+fringes&author=N.+Sekiguchi&author=D.+R.+Williams&author=D.+H.+Brainard&publication_year=1993&journal=J.+Opt.+Soc.+Am.+A&pages=2118-2133&doi=10.1364%2FJOSAA.10.002118)

22

B. Chen, W. Makous, D. R. Williams, Serial spatial filters in vision. *Vision Res.* **33**, 413–427 (1993).

[Crossref](https://doi.org/10.1016/0042-6989\(93\)90095-E)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/8447111/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Serial+spatial+filters+in+vision&author=B.+Chen&author=W.+Makous&author=D.+R.+Williams&publication_year=1993&journal=Vision+Res.&pages=413-427&doi=10.1016%2F0042-6989%2893%2990095-E&pmid=8447111)

*   [a \[...\] the cone’s spatial light gathering function](#body-ref-R22-1)
*   [b \[...\] half the cone inner segment diameter (ISD)](#body-ref-R22-2)

23

V. P. Pandiyan, X. Jiang, J. A. Kuchenbecker, R. Sabesan, Reflective mirror-based line-scan adaptive optics OCT for imaging retinal structure and function. *Biomed. Opt. Express* **12**, 5865–5880 (2021).

[Crossref](https://doi.org/10.1364/BOE.436337)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/34692221/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000692527800005)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Reflective+mirror-based+line-scan+adaptive+optics+OCT+for+imaging+retinal+structure+and+function&author=V.+P.+Pandiyan&author=X.+Jiang&author=J.+A.+Kuchenbecker&author=R.+Sabesan&publication_year=2021&journal=Biomed.+Opt.+Express&pages=5865-5880&doi=10.1364%2FBOE.436337&pmid=34692221)

*   [a \[...\] techniques in an AO-OCT system](#body-ref-R23-1)
*   [b \[...\] not yet to the smallest cells in the fovea](#body-ref-R23-2)
*   [c \[...\] on a separate AO-OCT instrument](#body-ref-R23-3)

24

W. Kurtenbach, C. E. Sternheim, L. Spillmann, Change in hue of spectral colors by dilution with white light (Abney effect). *J. Opt. Soc. Am. A* **1**, 365–372 (1984).

[GO TO REFERENCE](#body-ref-R24)

[Crossref](https://doi.org/10.1364/JOSAA.1.000365)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/6726489/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Change+in+hue+of+spectral+colors+by+dilution+with+white+light+%28Abney+effect%29&author=W.+Kurtenbach&author=C.+E.+Sternheim&author=L.+Spillmann&publication_year=1984&journal=J.+Opt.+Soc.+Am.+A&pages=365-372&doi=10.1364%2FJOSAA.1.000365&pmid=6726489)

25

J. C. Maxwell, *On the Theory of Three Primary Colours* (Cambridge Univ. Press, 2011), pp. 445-450.

[GO TO REFERENCE](#body-ref-R25)

[Google Scholar](https://scholar.google.com/scholar?q=J.+C.+Maxwell%2C+On+the+Theory+of+Three+Primary+Colours+%28Cambridge+Univ.+Press%2C+2011%29%2C+pp.+445-450.)

26

V. P. Pandiyan, S. Schleufer, E. Slezak, J. Fong, R. Upadhyay, A. Roorda, R. Ng, R. Sabesan, Characterizing cone spectral classification by optoretinography. *Biomed. Opt. Express* **13**, 6574–6594 (2022).

[Crossref](https://doi.org/10.1364/BOE.473608)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/36589563/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000917262200028)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Characterizing+cone+spectral+classification+by+optoretinography&author=V.+P.+Pandiyan&author=S.+Schleufer&author=E.+Slezak&author=J.+Fong&author=R.+Upadhyay&author=A.+Roorda&author=R.+Ng&author=R.+Sabesan&publication_year=2022&journal=Biomed.+Opt.+Express&pages=6574-6594&doi=10.1364%2FBOE.473608&pmid=36589563)

*   [a \[...\] not yet to the smallest cells in the fovea](#body-ref-R26-1)
*   [b \[...\] the Retina Map Alignment Tool \[described in](#body-ref-R26-2)

27

J. E. Vanston, A. E. Boehm, W. S. Tuten, A. Roorda, It’s not easy seeing green: The veridical perception of small spots. *J. Vis.* **23**, 2 (2023).

[GO TO REFERENCE](#body-ref-R27)

[Crossref](https://doi.org/10.1167/jov.23.5.2)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/37133838/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000994047800008)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=It%E2%80%99s+not+easy+seeing+green%3A+The+veridical+perception+of+small+spots&author=J.+E.+Vanston&author=A.+E.+Boehm&author=W.+S.+Tuten&author=A.+Roorda&publication_year=2023&journal=J.+Vis.&pages=2&doi=10.1167%2Fjov.23.5.2&pmid=37133838)

28

M. J. Greene, A. E. Boehm, J. E. Vanston, V. P. Pandiyan, R. Sabesan, W. S. Tuten, Unique yellow shifts for small and brief stimuli in the central retina. *J. Vis.* **24**, 2 (2024).

[GO TO REFERENCE](#body-ref-R28)

[Crossref](https://doi.org/10.1167/jov.24.6.2)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/38833255/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A001244308500002)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Unique+yellow+shifts+for+small+and+brief+stimuli+in+the+central+retina&author=M.+J.+Greene&author=A.+E.+Boehm&author=J.+E.+Vanston&author=V.+P.+Pandiyan&author=R.+Sabesan&author=W.+S.+Tuten&publication_year=2024&journal=J.+Vis.&pages=2&doi=10.1167%2Fjov.24.6.2&pmid=38833255)

29

J. Freeman, G. D. Field, P. H. Li, M. Greschner, D. E. Gunning, K. Mathieson, A. Sher, A. M. Litke, L. Paninski, E. P. Simoncelli, E. J. Chichilnisky, Mapping nonlinear receptive field structure in primate retina at single cone resolution. *eLife* **4**, e05241 (2015).

[GO TO REFERENCE](#body-ref-R29)

[Crossref](https://doi.org/10.7554/eLife.05241)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/26517879/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000373884300001)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Mapping+nonlinear+receptive+field+structure+in+primate+retina+at+single+cone+resolution&author=J.+Freeman&author=G.+D.+Field&author=P.+H.+Li&author=M.+Greschner&author=D.+E.+Gunning&author=K.+Mathieson&author=A.+Sher&author=A.+M.+Litke&author=L.+Paninski&author=E.+P.+Simoncelli&author=E.+J.+Chichilnisky&publication_year=2015&journal=eLife&pages=e05241&doi=10.7554%2FeLife.05241&pmid=26517879)

30

J. B. Demb, L. Haarsma, M. A. Freed, P. Sterling, Functional circuitry of the retinal ganglion cell’s nonlinear receptive field. *J. Neurosci.* **19**, 9756–9767 (1999).

[GO TO REFERENCE](#body-ref-R30)

[Crossref](https://doi.org/10.1523/JNEUROSCI.19-22-09756.1999)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10559385/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Functional+circuitry+of+the+retinal+ganglion+cell%E2%80%99s+nonlinear+receptive+field&author=J.+B.+Demb&author=L.+Haarsma&author=M.+A.+Freed&author=P.+Sterling&publication_year=1999&journal=J.+Neurosci.&pages=9756-9767&doi=10.1523%2FJNEUROSCI.19-22-09756.1999&pmid=10559385)

31

D. R. Coates, X. Jiang, J. A. Kuchenbecker, R. Sabesan, The Rayleigh limit of the parvocellular pathway. *Invest. Ophthalmol. Vis. Sci.* **60**, 1312 (2019).

[GO TO REFERENCE](#body-ref-R31)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000488628103093)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+Rayleigh+limit+of+the+parvocellular+pathway&author=D.+R.+Coates&author=X.+Jiang&author=J.+A.+Kuchenbecker&author=R.+Sabesan&publication_year=2019&journal=Invest.+Ophthalmol.+Vis.+Sci.&pages=1312)

32

A. M. Geller, P. A. Sieving, D. G. Green, Effect on grating identification of sampling with degenerate arrays. *J. Opt. Soc. Am. A* **9**, 472–477 (1992).

[GO TO REFERENCE](#body-ref-R32)

[Crossref](https://doi.org/10.1364/JOSAA.9.000472)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/1548555/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Effect+on+grating+identification+of+sampling+with+degenerate+arrays&author=A.+M.+Geller&author=P.+A.+Sieving&author=D.+G.+Green&publication_year=1992&journal=J.+Opt.+Soc.+Am.+A&pages=472-477&doi=10.1364%2FJOSAA.9.000472&pmid=1548555)

33

K. Ratnam, J. Carroll, T. C. Porco, J. L. Duncan, A. Roorda, Relationship between foveal cone structure and clinical measures of visual function in patients with inherited retinal degenerations. *Invest. Ophthalmol. Vis. Sci.* **54**, 5836–5847 (2013).

[GO TO REFERENCE](#body-ref-R33)

[Crossref](https://doi.org/10.1167/iovs.13-12557)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/23908179/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000325167200086)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Relationship+between+foveal+cone+structure+and+clinical+measures+of+visual+function+in+patients+with+inherited+retinal+degenerations&author=K.+Ratnam&author=J.+Carroll&author=T.+C.+Porco&author=J.+L.+Duncan&author=A.+Roorda&publication_year=2013&journal=Invest.+Ophthalmol.+Vis.+Sci.&pages=5836-5847&doi=10.1167%2Fiovs.13-12557&pmid=23908179)

34

K. Mancuso, W. W. Hauswirth, Q. Li, T. B. Connor, J. A. Kuchenbecker, M. C. Mauck, J. Neitz, M. Neitz, Gene therapy for red–green colour blindness in adult primates. *Nature* **461**, 784–787 (2009).

[GO TO REFERENCE](#body-ref-R34)

[Crossref](https://doi.org/10.1038/nature08401)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/19759534/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000270547500034)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Gene+therapy+for+red%E2%80%93green+colour+blindness+in+adult+primates&author=K.+Mancuso&author=W.+W.+Hauswirth&author=Q.+Li&author=T.+B.+Connor&author=J.+A.+Kuchenbecker&author=M.+C.+Mauck&author=J.+Neitz&author=M.+Neitz&publication_year=2009&journal=Nature&pages=784-787&doi=10.1038%2Fnature08401&pmid=19759534)

35

G. H. Jacobs, Photopigments and the dimensionality of animal color vision. *Neurosci. Biobehav. Rev.* **86**, 108–130 (2018).

[GO TO REFERENCE](#body-ref-R35)

[Crossref](https://doi.org/10.1016/j.neubiorev.2017.12.006)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/29224775/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000426224600010)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Photopigments+and+the+dimensionality+of+animal+color+vision&author=G.+H.+Jacobs&publication_year=2018&journal=Neurosci.+Biobehav.+Rev.&pages=108-130&doi=10.1016%2Fj.neubiorev.2017.12.006&pmid=29224775)

36

C. A. Curcio, K. R. Sloan, R. E. Kalina, A. E. Hendrickson, Human photoreceptor topography. *J. Comp. Neurol.* **292**, 497–523 (1990).

[Crossref](https://doi.org/10.1002/cne.902920402)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/2324310/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1990CR16200001)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Human+photoreceptor+topography&author=C.+A.+Curcio&author=K.+R.+Sloan&author=R.+E.+Kalina&author=A.+E.+Hendrickson&publication_year=1990&journal=J.+Comp.+Neurol.&pages=497-523&doi=10.1002%2Fcne.902920402&pmid=2324310)

*   [a \[...\] because of increasing spacing of cones](#body-ref-R36-1)
*   [b \[...\] respectively, consistent with Curcio et al.](#body-ref-R36-2)

37

W. M. Harmening, W. S. Tuten, A. Roorda, L. C. Sincich, Mapping the perceptual grain of the human retina. *J. Neurosci.* **34**, 5667–5677 (2014).

[GO TO REFERENCE](#body-ref-R37)

[Crossref](https://doi.org/10.1523/JNEUROSCI.5191-13.2014)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/24741057/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000334926000028)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Mapping+the+perceptual+grain+of+the+human+retina&author=W.+M.+Harmening&author=W.+S.+Tuten&author=A.+Roorda&author=L.+C.+Sincich&publication_year=2014&journal=J.+Neurosci.&pages=5667-5677&doi=10.1523%2FJNEUROSCI.5191-13.2014&pmid=24741057)

38

D. A. Atchison, G. Smith, Chromatic dispersions of the ocular media of human eyes. *J. Opt. Soc. Am. A* **22**, 29–37 (2005).

[GO TO REFERENCE](#body-ref-R38)

[Crossref](https://doi.org/10.1364/JOSAA.22.000029)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000226397300003)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Chromatic+dispersions+of+the+ocular+media+of+human+eyes&author=D.+A.+Atchison&author=G.+Smith&publication_year=2005&journal=J.+Opt.+Soc.+Am.+A&pages=29-37&doi=10.1364%2FJOSAA.22.000029)

39

J. Liang, D. R. Williams, D. T. Miller, Supernormal vision and high-resolution retinal imaging through adaptive optics. *J. Opt. Soc. Am. A* **14**, 2884–2892 (1997).

[GO TO REFERENCE](#body-ref-R39)

[Crossref](https://doi.org/10.1364/JOSAA.14.002884)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Supernormal+vision+and+high-resolution+retinal+imaging+through+adaptive+optics&author=J.+Liang&author=D.+R.+Williams&author=D.+T.+Miller&publication_year=1997&journal=J.+Opt.+Soc.+Am.+A&pages=2884-2892&doi=10.1364%2FJOSAA.14.002884)

40

Q. Yang, D. W. Arathorn, P. Tiruveedhula, C. R. Vogel, A. Roorda, Design of an integrated hardware interface for AOSLO image capture and cone-targeted stimulus delivery. *Opt. Express* **18**, 17841–17858 (2010).

[GO TO REFERENCE](#body-ref-R40)

[Crossref](https://doi.org/10.1364/OE.18.017841)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/20721171/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000281054400028)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Design+of+an+integrated+hardware+interface+for+AOSLO+image+capture+and+cone-targeted+stimulus+delivery&author=Q.+Yang&author=D.+W.+Arathorn&author=P.+Tiruveedhula&author=C.+R.+Vogel&author=A.+Roorda&publication_year=2010&journal=Opt.+Express&pages=17841-17858&doi=10.1364%2FOE.18.017841&pmid=20721171)

41

A. E. Boehm, C. M. Privitera, B. P. Schmidt, A. Roorda, Transverse chromatic offsets with pupil displacements in the human eye: Sources of variability and methods for real-time correction. *Biomed. Opt. Express* **10**, 1691–1706 (2019).

[GO TO REFERENCE](#body-ref-R41)

[Crossref](https://doi.org/10.1364/BOE.10.001691)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/31061763/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000462887400013)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Transverse+chromatic+offsets+with+pupil+displacements+in+the+human+eye%3A+Sources+of+variability+and+methods+for+real-time+correction&author=A.+E.+Boehm&author=C.+M.+Privitera&author=B.+P.+Schmidt&author=A.+Roorda&publication_year=2019&journal=Biomed.+Opt.+Express&pages=1691-1706&doi=10.1364%2FBOE.10.001691&pmid=31061763)

42

J. Shenoy, J. Fong, J. Tan, A. Roorda, R. Ng, “R-SLAM: Optimizing eye tracking from rolling shutter video of the retina,” in *Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision* (IEEE, 2021), pp. 4852–4861.

[Google Scholar](https://scholar.google.com/scholar?q=J.+Shenoy%2C+J.+Fong%2C+J.+Tan%2C+A.+Roorda%2C+R.+Ng%2C+%E2%80%9CR-SLAM%3A+Optimizing+eye+tracking+from+rolling+shutter+video+of+the+retina%2C%E2%80%9D+in+Proceedings+of+the+2021+IEEE%2FCVF+International+Conference+on+Computer+Vision+%28IEEE%2C+2021%29%2C+pp.+4852%E2%80%934861.)

*   [a \[...\] a global optimization algorithm, R-SLAM](#body-ref-R42-1)
*   [b \[...\] is to modify the R-SLAM algorithm](#body-ref-R42-2)

43

S. B. Stevenson, A. Roorda, “Correcting for miniature eye movements in high resolution scanning laser ophthalmoscopy” in *SPIE Proceedings 5688* (SPIE, 2005), pp. 145–151.

[Google Scholar](https://scholar.google.com/scholar?q=S.+B.+Stevenson%2C+A.+Roorda%2C+%E2%80%9CCorrecting+for+miniature+eye+movements+in+high+resolution+scanning+laser+ophthalmoscopy%E2%80%9D+in+SPIE+Proceedings+5688+%28SPIE%2C+2005%29%2C+pp.+145%E2%80%93151.)

*   [a \[...\] (NCC) strip-based matching method](#body-ref-R43-1)
*   [b \[...\] of the independently tracked halves](#body-ref-R43-2)

44

M. A. Fischler, R. C. Bolles, Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. *Commun. ACM* **24**, 381–395 (1981).

[GO TO REFERENCE](#body-ref-R44)

[Crossref](https://doi.org/10.1145/358669.358692)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Random+sample+consensus%3A+A+paradigm+for+model+fitting+with+applications+to+image+analysis+and+automated+cartography&author=M.+A.+Fischler&author=R.+C.+Bolles&publication_year=1981&journal=Commun.+ACM&pages=381-395&doi=10.1145%2F358669.358692)

45

J. Fong, “How to See Impossible Colors: First Steps Toward the Oz Vision Display,” thesis, University of California, Berkeley (2021).

[GO TO REFERENCE](#body-ref-R45)

[Google Scholar](https://scholar.google.com/scholar?q=J.+Fong%2C+%E2%80%9CHow+to+See+Impossible+Colors%3A+First+Steps+Toward+the+Oz+Vision+Display%2C%E2%80%9D+thesis%2C+University+of+California%2C+Berkeley+%282021%29.)

46

W. M. Harmening, P. Tiruveedhula, A. Roorda, L. C. Sincich, Measurement and correction of transverse chromatic offsets for multi-wavelength retinal microscopy in the living eye. *Biomed. Opt. Express* **3**, 2066–2077 (2012).

[Crossref](https://doi.org/10.1364/BOE.3.002066)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/23024901/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000308861100010)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Measurement+and+correction+of+transverse+chromatic+offsets+for+multi-wavelength+retinal+microscopy+in+the+living+eye&author=W.+M.+Harmening&author=P.+Tiruveedhula&author=A.+Roorda&author=L.+C.+Sincich&publication_year=2012&journal=Biomed.+Opt.+Express&pages=2066-2077&doi=10.1364%2FBOE.3.002066&pmid=23024901)

*   [a \[...\] improve on previous image-based procedures](#body-ref-R46-1)
*   [b \[...\] TCA is highly dependent on pupil location](#body-ref-R46-2)

47

D. H. Brainard, The psychophysics toolbox. *Spat. Vis.* **10**, 433–436 (1997).

[GO TO REFERENCE](#body-ref-R47)

[Crossref](https://doi.org/10.1163/156856897X00357)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/9176952/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1997WZ53600014)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+psychophysics+toolbox&author=D.+H.+Brainard&publication_year=1997&journal=Spat.+Vis.&pages=433-436&doi=10.1163%2F156856897X00357&pmid=9176952)

48

D. G. Pelli, The videotoolbox software for visual psychophysics: Transforming numbers into movies. *Spat. Vis.* **10**, 437–442 (1997).

[Crossref](https://doi.org/10.1163/156856897X00366)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/9176953/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1997WZ53600015)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+videotoolbox+software+for+visual+psychophysics%3A+Transforming+numbers+into+movies&author=D.+G.+Pelli&publication_year=1997&journal=Spat.+Vis.&pages=437-442&doi=10.1163%2F156856897X00366&pmid=9176953)

49

M. Kleiner, D. Brainard, D. Pelli, A. Ingling, R. Murray, C. Broussard, What’s new in psychtoolbox-3. *Perception* **36**, 1–16 (2007).

[GO TO REFERENCE](#body-ref-R49)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000250594600049)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=What%E2%80%99s+new+in+psychtoolbox-3&author=M.+Kleiner&author=D.+Brainard&author=D.+Pelli&author=A.+Ingling&author=R.+Murray&author=C.+Broussard&publication_year=2007&journal=Perception&pages=1-16)

50

“Amendment 1 - Multimedia systems and equipment - Colour measurement and management - Part 2-1: Colour management - Default RGB colour space – sRGB” (Tech. Rep. IEC 61966-2- 1:1999/AMD1:2003, International Electrotechnical Commission, 2003).

[GO TO REFERENCE](#body-ref-R50)

[Google Scholar](https://scholar.google.com/scholar?q=%E2%80%9CAmendment+1+-+Multimedia+systems+and+equipment+-+Colour+measurement+and+management+-+Part+2-1%3A+Colour+management+-+Default+RGB+colour+space+%E2%80%93+sRGB%E2%80%9D+%28Tech.+Rep.+IEC+61966-2-+1%3A1999%2FAMD1%3A2003%2C+International+Electrotechnical+Commission%2C+2003%29.)

51

“Colorimetry — Part 4: CIE 1976 l\*a\*b\* colour space” (Tech. Rep. ISO/CIE 11664-4:2019, International Commission on Illumination, 2019).

[Google Scholar](https://scholar.google.com/scholar?q=%E2%80%9CColorimetry+%E2%80%94+Part+4%3A+CIE+1976+l%2Aa%2Ab%2A+colour+space%E2%80%9D+%28Tech.+Rep.+ISO%2FCIE+11664-4%3A2019%2C+International+Commission+on+Illumination%2C+2019%29.)

*   [a \[...\] ) colors with matching CIELAB hue](#body-ref-R51-1)
*   [b \[...\] in standards such as CIELAB and ΔEab\*](#body-ref-R51-2)

52

D. L. MacAdam, Visual sensitivities to color differences in daylight. *J. Opt. Soc. Am.* **32**, 247–274 (1942).

[GO TO REFERENCE](#body-ref-R52)

[Crossref](https://doi.org/10.1364/JOSA.32.000247)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Visual+sensitivities+to+color+differences+in+daylight&author=D.+L.+MacAdam&publication_year=1942&journal=J.+Opt.+Soc.+Am.&pages=247-274&doi=10.1364%2FJOSA.32.000247)

53

M. Mahy, L. Van Eycken, A. Oosterlinck, Evaluation of uniform color spaces developed after the adoption of CIELAB and CIELUV. *Color Res. Appl.* **19**, 105–121 (1994).

[GO TO REFERENCE](#body-ref-R53)

[Crossref](https://doi.org/10.1111/j.1520-6378.1994.tb00070.x)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Evaluation+of+uniform+color+spaces+developed+after+the+adoption+of+CIELAB+and+CIELUV&author=M.+Mahy&author=L.+Van+Eycken&author=A.+Oosterlinck&publication_year=1994&journal=Color+Res.+Appl.&pages=105-121&doi=10.1111%2Fj.1520-6378.1994.tb00070.x)

54

W. M. Harmening, L. C. Sincich, in *Adaptive Optics for Photoreceptor-Targeted Psychophysics*, (Springer International Publishing, Cham, 2019), pp. 359–375.

[GO TO REFERENCE](#body-ref-R54)

[Google Scholar](https://scholar.google.com/scholar?q=W.+M.+Harmening%2C+L.+C.+Sincich%2C+in+Adaptive+Optics+for+Photoreceptor-Targeted+Psychophysics%2C+%28Springer+International+Publishing%2C+Cham%2C+2019%29%2C+pp.+359%E2%80%93375.)

Show all references

### Submit a Response to This Article

×

##### Compose eLetter

Title:\*

Title is required

Contents:

##### Contributors

remove contributor

First name:

Last name:

Email:

Role/occupation:

affiliation:

add another contributor

##### Statement of Competing Interests

Competing interests?

YES

NO

Please describe the competing interests\*

Fields marked with \* are required

CANCELSUBMIT

## (0)eLetters

eLetters is a forum for ongoing peer review. eLetters are not edited, proofread, or indexed, but they are screened. eLetters should provide substantive and scholarly commentary on the article. Neither embedded figures nor equations with special characters can be submitted, and we discourage the use of figures and equations within eLetters in general. If a figure or equation is essential, please include within the text of the eLetter a link to the figure, equation, or full text with special characters at a public repository with versioning, such as Zenodo. Please read our [Terms of Service](/content/page/terms-service) before submitting an eLetter.

[Log In to Submit a Response](/action/ssostart?redirectUri=/doi/full/10.1126/sciadv.adu1052)

No eLetters have been published for this article yet.

[SHOW ALL eLETTERS](#itemsCollapseItems)

## Information & Authors

InformationAuthors

### Information

#### Published In

![](/cms/asset/731c4cd6-8da9-460f-9b34-7fd3893173b5/sciadv.2025.11.issue-16.largecover.jpg)

Science Advances

Volume 11 | Issue 16  
April 2025

#### Copyright

Copyright © 2025 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License 4.0 (CC BY).

[https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)

This is an open-access article distributed under the terms of the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.

#### Article versions

[](# "Check for updates on crossmark")

#### Submission history

**Received**: 30 October 2024

**Accepted**: 17 March 2025

#### Permissions

Request permissions for this article.

[Request permissions](https://s100.copyright.com/AppDispatchServlet?publisherName=AAAS&publication=scienceadvances&title=Novel+color+via+stimulation+of+individual+photoreceptors+at+population+scale&publicationDate=2025-04-18&author=James+Fong%2C+Hannah+K.+Doyle%2C+Congli+Wang%2C+Alexandra+E.+Boehm%2C+et+al.&contentID=10.1126%2Fsciadv.adu1052&volumeNum=11&issueNum=16&oa=cc-by&orderBeanReset=true)

#### Acknowledgments

We thank A. Kotani for graphics used in movie S1; B. Wendel for assistance in cone classification; R. Weber from Montana State University for FPGA programming; J. Shenoy for implementation of R-SLAM; R. Upadhyay for implementation of the Retina Map Alignment Tool; A. Aikawa, E. Alexander, H. Johnson, L. Y. Kat, J. Ku, P. Manohar, V. Ramakrishnan, A. Sabnis, U. Singhal, S. Sun, J. Tan, J. Zhang, and Y. Zong for assistance in preliminary system engineering; A. Belsten, D. Brainard, N. Jennings, A. Kotani, J. Lee, B. Olshausen, F. Rieke, V. N. Srivastava, A. Thakrar, and B. Wandell for reading and commenting on this manuscript; and P. Bharadwaj and S. Schleufer for additional support. C.W., A.E.B., S.R.H., B.P.S., and J.E.V. completed this work entirely at UC Berkeley, and are now, respectively, affiliated with Princeton University, Google Inc., Rochester Institute of Technology, Etsy Inc., and Exponent Inc.

**Funding:** This work was supported by a Hellman Fellowship (R.N.), FHL Vive Center Seed Grant (R.N.), Air Force Office of Scientific Research grant FA9550-20-1-0195 (J.F., C.W., W.S.T., A.R., and R.N.), Air Force Office of Scientific Research grant FA9550-21-1-0230 (J.F., H.K.D., C.W., A.E.B., S.R.H., V.P.P., W.S.T., R.S., A.R., and R.N.), National Institutes of Health grant R01EY023591 (A.E.B., B.P.S., P.T., J.E.V., W.S.T., A.R., and R.N.), National Institutes of Health grant R01EY029710 (V.P.P. and R.S.), National Institutes of Health grant U01EY032055 (V.P.P., R.S., and A.R.), and a Burroughs Wellcome Fund Career Award at the Scientific Interface (R.S.).

**Author contributions:** Conceptualization: J.F., H.K.D., A.E.B., V.P.P., B.P.S., A.R., and R.N. Methodology: J.F., H.K.D., C.W., A.E.B., V.P.P., B.P.S., R.S., A.R., and R.N. Investigation: J.F., H.K.D., C.W., A.E.B., S.R.H., V.P.P., B.P.S., J.E.V., W.S.T., R.S., A.R., and R.N. Formal analysis: J.F., H.K.D., A.E.B., V.P.P., R.S., and R.N. Resources: V.P.P., P.T., W.S.T., R.S., A.R., and R.N. Funding acquisition: W.S.T., R.S., A.R., and R.N. Project administration: J.F., H.K.D., R.S., A.R., and R.N. Supervision: W.S.T., R.S., A.R., and R.N. Visualization: J.F., H.K.D., A.R., and R.N. Data curation: J.F., H.K.D., C.W., V.P.P., A.R., and R.N. Validation: J.F., H.K.D., C.W., V.P.P., A.R., and R.N. Software: J.F., H.K.D., C.W., A.E.B., S.R.H., V.P.P., B.P.S., P.T., and R.N. Writing–original draft: J.F., H.K.D., C.W., and R.N. Writing–review and editing: J.F., H.K.D., A.E.B., S.R.H., V.P.P., B.P.S., J.E.V., W.S.T., A.R., and R.N.

**Competing interests:** The Regents of the University of California has filed a patent for cell-by-cell retina stimulation, for which R.N., A.R., and B.P.S. are inventors (WO2020086612A1). V.P.P. and R.S. have filed a US patent application describing the technology for the linescan OCT for optoretinography. All other authors declare that they have no competing interests.

**Data and materials availability:** Code and data underlying the study are deposited in Dryad at [https://doi.org/10.5061/dryad.pc866t206](https://doi.org/10.5061/dryad.pc866t206). All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials.

### Authors

#### AffiliationsExpand All

##### James Fong[†](#afn1) [https://orcid.org/0009-0009-5106-8404](https://orcid.org/0009-0009-5106-8404)

Department of Electrical Engineering & Computer Sciences, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Software, Validation, Visualization, Writing - original draft, and Writing - review & editing.

[View all articles by this author](/authored-by/Fong/James)

##### Hannah K. Doyle[†](#afn1) [https://orcid.org/0009-0003-3101-8588](https://orcid.org/0009-0003-3101-8588)

Department of Electrical Engineering & Computer Sciences, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Software, Validation, Visualization, Writing - original draft, and Writing - review & editing.

[View all articles by this author](/authored-by/Doyle/Hannah+K)

##### Congli Wang[†](#afn1) [https://orcid.org/0000-0003-3112-2820](https://orcid.org/0000-0003-3112-2820)

Department of Electrical Engineering & Computer Sciences, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Data curation, Investigation, Methodology, Software, Validation, and Writing - original draft.

[View all articles by this author](/authored-by/Wang/Congli)

##### Alexandra E. Boehm

Herbert Wertheim School of Optometry & Vision Science, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Conceptualization, Formal analysis, Investigation, Methodology, Software, and Writing - review & editing.

[View all articles by this author](/authored-by/Boehm/Alexandra+E)

##### Sofie R. Herbeck [https://orcid.org/0009-0000-2044-7895](https://orcid.org/0009-0000-2044-7895)

Department of Electrical Engineering & Computer Sciences, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Investigation, Software, and Writing - review & editing.

[View all articles by this author](/authored-by/Herbeck/Sofie+R)

##### Vimal Prabhu Pandiyan [https://orcid.org/0000-0002-6996-0355](https://orcid.org/0000-0002-6996-0355)

Department of Ophthalmology, University of Washington School of Medicine, Seattle, WA 98195, USA.

Roles: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Resources, Software, Validation, and Writing - review & editing.

[View all articles by this author](/authored-by/Pandiyan/Vimal+Prabhu)

##### Brian P. Schmidt [https://orcid.org/0000-0003-3460-0605](https://orcid.org/0000-0003-3460-0605)

Herbert Wertheim School of Optometry & Vision Science, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Conceptualization, Investigation, Methodology, Software, and Writing - review & editing.

[View all articles by this author](/authored-by/Schmidt/Brian+P)

##### Pavan Tiruveedhula [https://orcid.org/0009-0001-5672-4046](https://orcid.org/0009-0001-5672-4046)

Herbert Wertheim School of Optometry & Vision Science, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Resources and Software.

[View all articles by this author](/authored-by/Tiruveedhula/Pavan)

##### John E. Vanston

Herbert Wertheim School of Optometry & Vision Science, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Investigation and Writing - review & editing.

[View all articles by this author](/authored-by/Vanston/John+E)

##### William S. Tuten [https://orcid.org/0000-0003-3856-017X](https://orcid.org/0000-0003-3856-017X)

Herbert Wertheim School of Optometry & Vision Science, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Funding acquisition, Investigation, Resources, Supervision, and Writing - review & editing.

[View all articles by this author](/authored-by/Tuten/William+S)

##### Ramkumar Sabesan [https://orcid.org/0000-0003-0895-7037](https://orcid.org/0000-0003-0895-7037)

Department of Ophthalmology, University of Washington School of Medicine, Seattle, WA 98195, USA.

Roles: Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, and Supervision.

[View all articles by this author](/authored-by/Sabesan/Ramkumar)

##### Austin Roorda [https://orcid.org/0000-0002-3785-0848](https://orcid.org/0000-0002-3785-0848)

Herbert Wertheim School of Optometry & Vision Science, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Conceptualization, Data curation, Funding acquisition, Investigation, Methodology, Project administration, Resources, Supervision, Validation, Visualization, and Writing - review & editing.

[View all articles by this author](/authored-by/Roorda/Austin)

##### Ren Ng[\*](#cor1) [https://orcid.org/0009-0007-2207-021X](https://orcid.org/0009-0007-2207-021X) [ren@berkeley.edu](mailto:ren@berkeley.edu)

Department of Electrical Engineering & Computer Sciences, University of California, Berkeley, Berkeley, CA 94720, USA.

Roles: Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing - original draft, and Writing - review & editing.

[View all articles by this author](/authored-by/Ng/Ren)

#### Funding Information

[National Institutes of Health](http://dx.doi.org/10.13039/100000002): R01EY023591

[National Institutes of Health](http://dx.doi.org/10.13039/100000002): R01EY029710

[National Institutes of Health](http://dx.doi.org/10.13039/100000002): U01EY032055

[Air Force Office of Scientific Research](http://dx.doi.org/10.13039/100000181): FA9550-20-1-0195

[Air Force Office of Scientific Research](http://dx.doi.org/10.13039/100000181): FA9550-21-1-0230

[Burroughs Wellcome Fund](http://dx.doi.org/10.13039/100000861)

The Society of Hellman Fellows

FHL Vive Center

#### Notes

\*

Corresponding author. Email: [ren@berkeley.edu](mailto:ren@berkeley.edu)

†

These authors contributed equally to this work.

## Metrics & Citations

MetricsCitations

### Metrics

#### Article Usage

Article Metrics

*   [Downloads](# "Downloads")
*   Citations

No data available.

05,00010,00015,00020 Apr 202527 Apr 202504 May 202511 May 202518 May 2025

23,937

0

*   [Total](#)
*   [First 30 Days](#)

Total number of downloads for the first 30 days after content publication

**Note:** The article usage is presented with a three- to four-day delay and will update daily once available. Due to this delay, usage data will not appear immediately following publication.

Citation information is sourced from [Crossref Cited-by](https://www.crossref.org/services/cited-by/ "Follow link") service.

*   0 citation in Scopus
    
*   0 citation in Web of Science
    

#### Altmetrics

[![Article has an altmetric score of 1422](https://badges.altmetric.com/?size=240&score=1422&types=mbrtttfu)](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances)

[See more details](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances)

[Picked up by **143** news outlets](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances&tab=news)

[Blogged by **3**](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances&tab=blogs)

[Posted by **258** X users](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances&tab=twitter)

[On **4** Facebook pages](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances&tab=facebook)

[Reddited by **3**](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances&tab=reddit)

[Referenced by **159** Bluesky users](https://www.altmetric.com/details.php?domain=www.science.org&citation_id=176249442&template=scienceadvances&tab=bluesky)

### Citations

#### Cite as

*   James Fong *et al.*

,

Novel color via stimulation of individual photoreceptors at population scale.*Sci. Adv.***11**,eadu1052(2025).DOI:[10.1126/sciadv.adu1052](https://doi.org/10.1126/sciadv.adu1052)

#### Export citation

Select the format you want to export the citation of this publication.

Please select one from the list RIS (ProCite, Reference Manager) EndNote BibTex Medlars RefWorks

 Direct import

Citation information is sourced from [Crossref Cited-by](https://www.crossref.org/services/cited-by/ "Follow link") service.

## View Options

### View options

#### PDF format

Download this article as a PDF file

[Download PDF](/doi/pdf/10.1126/sciadv.adu1052?download=true)

## Figures

[![](/cms/10.1126/sciadv.adu1052/asset/676ebf10-6471-4c5a-9b79-dfe102d331a1/assets/images/large/sciadv.adu1052-f1.jpg)](#F1)

Fig. 1. Overview of principle and prototype system.

(**A**) System inputs. (i) Retina map of 103 cone cells preclassified by spectral type ([*7*](#core-R7)). (ii) Target visual percept (here, a video of a child, see movie S1 at 1:04). (iii) Infrared cellular-scale imaging of the retina with 60-frames-per-second rolling shutter. Fixational eye movement is visible over the three frames shown. (**B**) System outputs. (iv) Real-time per-cone target activation levels to reproduce the target percept, computed by: extracting eye motion from the input video relative to the retina map; identifying the spectral type of every cone in the field of view; computing the per-cone activation the target percept would have produced. (v) Intensities of visible-wavelength 488-nm laser microdoses at each cone required to achieve its target activation level. (**C**) Infrared imaging and visible-wavelength stimulation are physically accomplished in a raster scan across the retinal region using AOSLO. By modulating the visible-wavelength beam’s intensity, the laser microdoses shown in (v) are delivered. Drawing adapted with permission \[Harmening and Sincich ([*54*](#core-R54))\]. (**D**) Examples of target percepts with corresponding cone activations and laser microdoses, ranging from colored squares to complex imagery. Teal-striped regions represent the color “olo” of stimulating only M cones.

[GO TO FIGURE](#F1)[OPEN IN VIEWER](#F1)

[![](/cms/10.1126/sciadv.adu1052/asset/d0883609-d37f-450f-b297-5a934fddff67/assets/images/large/sciadv.adu1052-f2.jpg)](#F2)

Fig. 2. Theoretical model of Oz color gamut as a function of fractional leak and stimulation wavelength.

(**A**) Gamut shrinks from the full *lms* chromaticity triangle to the stimulation wavelength (open circle) as the fractional light leak grows; note that this fraction depends on the intercone spacing, which varies across the retina. The colored region is the gamut of natural human colors. (**B**) Gamut varies in chromaticity, position, and shape as a function of stimulation wavelength. For readability, extra copies of the gamuts for 543 and 589 nm are drawn next to the *lm* edge.

[GO TO FIGURE](#F2)[OPEN IN VIEWER](#F2)

[![](/cms/10.1126/sciadv.adu1052/asset/fb500790-9651-44e9-998b-9df4d8cd125c/assets/images/large/sciadv.adu1052-f3.jpg)](#F3)

Fig. 3. Color matching of Oz colored squares produced by cone-by-cone stimulation.

(**A** to **D**) Each *lms* chromaticity triangle plots color matches for one subject with the indicated stimulation wavelength and type of matching color system (RGB projector, or tunable near-monochromatic laser and projector white). Target colors are specified as (L, M, and S) triplets, which are the relative light intensity levels directed to each cone class. Color matches to different target colors are denoted with differently colored markers. Each triangle also plots: color matches for randomly interleaved jitter control condition \[see (E) and the “Design of prototype” section\]; coordinates of the stimulation wavelength; natural color gamut of human vision; gamut of the matching color system and its whitepoint; and perceptual uncertainty ellipses for the average color matches (projected JND ellipsoid at the coordinates of the “positive” component of the color match, computed from CIELAB/ΔEab\*, scaled three times the actual size; see the “Plotting perceptual uncertainty in matching” section in Materials and Methods). Ellipses not visible are smaller than their associated markers. (**E**) Illustration of the control condition randomly interleaved into all experiments: microdose target locations are randomly jittered by two intercone spacings in Oz stimuli that are otherwise identical to the experimental condition.

[GO TO FIGURE](#F3)[OPEN IN VIEWER](#F3)

[![](/cms/10.1126/sciadv.adu1052/asset/570cf67c-7982-4046-9ca3-0b9d0965b71f/assets/images/large/sciadv.adu1052-f4.jpg)](#F4)

Fig. 4. Image and video recognition experiments.

We tested subjects’ ability to recognize image and video content consisting of Oz colors: (**A**) a 4-alternative forced choice (4-AFC) line orientation recognition task, and (**B**), a 2-AFC rotation direction task experiments. Oz stimuli consisted of equiluminant red lines and disks presented on an olo background, as depicted. The bar graphs show individual subject performance over 20 trials per condition and average accuracy across five subjects with 95% confidence intervals. In experimental conditions with Oz microdoses delivered experimentally (blue bars), subjects are able to accurately identify line orientation and rotation direction. In control-group stimuli (gray bars), where cone-targeting is compromised through jittering microdose target locations, task accuracy is reduced to guessing rate as indicated by the dashed lines.

[GO TO FIGURE](#F4)[OPEN IN VIEWER](#F4)

[![](/cms/10.1126/sciadv.adu1052/asset/c22483c4-b38d-46d4-a57a-69f6ce368513/assets/images/large/sciadv.adu1052-f5.jpg)](#F5)

Fig. 5. Subject’s view during color matching experiment.

Left shows the experimental view. Right shows an example of the multicolored mosaics shown for a periodic 15-s “refresh period.”

[GO TO FIGURE](#F5)[OPEN IN VIEWER](#F5)

[![](/cms/10.1126/sciadv.adu1052/asset/fa2481cf-1382-40a7-b61c-0188477622b2/assets/images/large/sciadv.adu1052-f6.jpg)](#F6)

Fig. 6. Computation of perceptual uncertainty ellipses.

(**A**) Human perceptual JND ellipsoids \[e.g., *E*(*c*) at color *c*\] are long and skinny, pointing at the origin in LMS Cartesian space. They project to ellipses in the (l,m,s)\=(L,M,S)L+M+S chromaticity triangle as shown. (**B**) Vector math for computing the coordinates of a color match *c**m* = *c**p* − *c**n*, where *c**p* is the “positive” color seen by subject at matching, and *c**n* is the “negative” light added to the test color to enable a match. (**C**) In color matching, because the color seen by the subject at the time of match submission is the “positive” color *c**p*, the perceptual uncertainty of the inferred color match *c**m* is the ellipsoid *E*(*c**p*), recentered on *c**m*, as shown. If *c**n* is non-negative, as shown, the ellipsoid recentered on *c**m*, *E*(*c**p*) − *c**n*, no longer points at the origin, and projects to a nonlinearly enlarged ellipse in the *lms* triangle. Therefore, it is desirable to minimize the “negative” light required to achieve a color match, as accomplished with the tunable monochromatic light source used for matching olo (see [Fig. 3Opens in image viewer](#F3)).

[GO TO FIGURE](#F6)[OPEN IN VIEWER](#F6)

## Tables

## Media

## Share

### Share

#### Copy the article link

https://www.science.org/doi/10.1126/sciadv.adu1052

Copy Link

Copied!

Copying failed.

#### Share on social media

[Facebook](# "Share on Facebook")[X](# "Share on X")[LinkedIn](# "Share on LinkedIn")[Reddit](# "Share on Reddit")[WeChat](# "Share on WeChat")[WhatsApp](# "Share on WhatsApp")[Bluesky](# "Share on Bluesky")[email](# "Share on email")

## References

### References

1

M. Fairchild, “Colorimetry” in *Color Appearance Models*, (John Wiley & Sons, 2013), pp. 56–84.

[Google Scholar](https://scholar.google.com/scholar?q=M.+Fairchild%2C+%E2%80%9CColorimetry%E2%80%9D+in+Color+Appearance+Models%2C+%28John+Wiley+%26+Sons%2C+2013%29%2C+pp.+56%E2%80%9384.)

*   [a \[...\] bounded color gamut of natural human vision](#body-ref-R1-1)
*   [b \[...\] the retina. As usual with color matching](#body-ref-R1-2)

2

A. Stockman, L. T. Sharpe, C. Fach, The spectral sensitivity of the human short-wavelength sensitive cones derived from thresholds and color matches. *Vision Res.* **39**, 2901–2927 (1999).

[Crossref](https://doi.org/10.1016/S0042-6989\(98\)00225-9)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10492818/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+spectral+sensitivity+of+the+human+short-wavelength+sensitive+cones+derived+from+thresholds+and+color+matches&author=A.+Stockman&author=L.+T.+Sharpe&author=C.+Fach&publication_year=1999&journal=Vision+Res.&pages=2901-2927&doi=10.1016%2FS0042-6989%2898%2900225-9&pmid=10492818)

*   [a \[...\] S cones and overlaps completely with them](#body-ref-R2-1)
*   [b \[...\] Stockman and Sharpe human cone responses](#body-ref-R2-2)

3

A. Stockman, L. T. Sharpe, The spectral sensitivities of the middle- and long-wavelength-sensitive cones derived from measurements in observers of known genotype. *Vision Res.* **40**, 1711–1737 (2000).

[Crossref](https://doi.org/10.1016/S0042-6989\(00\)00021-3)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10814758/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+spectral+sensitivities+of+the+middle-+and+long-wavelength-sensitive+cones+derived+from+measurements+in+observers+of+known+genotype&author=A.+Stockman&author=L.+T.+Sharpe&publication_year=2000&journal=Vision+Res.&pages=1711-1737&doi=10.1016%2FS0042-6989%2800%2900021-3&pmid=10814758)

*   [a \[...\] S cones and overlaps completely with them](#body-ref-R3-1)
*   [b \[...\] Stockman and Sharpe human cone responses](#body-ref-R3-2)

4

R. Sabesan, B. P. Schmidt, W. S. Tuten, A. Roorda, The elementary representation of spatial and color vision in the human retina. *Sci. Adv.* **2**, e1600797 (2016).

[Crossref](/servlet/linkout?suffix=e_1_3_2_5_2&dbid=4&doi=10.1126%2Fsciadv.adu1052&key=10.1126%2Fsciadv.1600797&site=aaas-site)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/27652339/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000383734400014)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+elementary+representation+of+spatial+and+color+vision+in+the+human+retina&author=R.+Sabesan&author=B.+P.+Schmidt&author=W.+S.+Tuten&author=A.+Roorda&publication_year=2016&journal=Sci.+Adv.&pages=e1600797&doi=10.1126%2Fsciadv.1600797&pmid=27652339)

*   [a \[...\] M cones is targeting light to only one](#body-ref-R4-1)
*   [b \[...\] that builds upon the cone-targeted methods](#body-ref-R4-2)
*   [c \[...\] to contribute to a stable color percept](#body-ref-R4-3)

5

H. Hofer, B. Singer, D. R. Williams, Different sensations from cones with the same photopigment. *J. Vis.* **5**, 444–454 (2005).

[GO TO REFERENCE](#body-ref-R5)

[Crossref](https://doi.org/10.1167/5.5.5)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/16097875/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000229664000005)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Different+sensations+from+cones+with+the+same+photopigment&author=H.+Hofer&author=B.+Singer&author=D.+R.+Williams&publication_year=2005&journal=J.+Vis.&pages=444-454&doi=10.1167%2F5.5.5&pmid=16097875)

6

B. P. Schmidt, A. E. Boehm, K. G. Foote, A. Roorda, The spectral identity of foveal cones is preserved in hue perception. *J. Vis.* **18**, 19 (2018).

[Crossref](https://doi.org/10.1167/18.11.19)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/30372729/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000451282200019)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+spectral+identity+of+foveal+cones+is+preserved+in+hue+perception&author=B.+P.+Schmidt&author=A.+E.+Boehm&author=K.+G.+Foote&author=A.+Roorda&publication_year=2018&journal=J.+Vis.&pages=19&doi=10.1167%2F18.11.19&pmid=30372729)

7

B. P. Schmidt, R. Sabesan, W. S. Tuten, J. Neitz, A. Roorda, Sensations from a single M-cone depend on the activity of surrounding S-cones. *Sci. Rep.* **8**, 8561 (2018).

[Crossref](https://doi.org/10.1038/s41598-018-26754-1)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/29867090/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000434011100048)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Sensations+from+a+single+M-cone+depend+on+the+activity+of+surrounding+S-cones&author=B.+P.+Schmidt&author=R.+Sabesan&author=W.+S.+Tuten&author=J.+Neitz&author=A.+Roorda&publication_year=2018&journal=Sci.+Rep.&pages=8561&doi=10.1038%2Fs41598-018-26754-1&pmid=29867090)

*   [a \[...\] cone cells preclassified by spectral type](#body-ref-R7-1)
*   [b \[...\] M cones is targeting light to only one](#body-ref-R7-2)
*   [c \[...\] to contribute to a stable color percept](#body-ref-R7-3)

8

B. P. Schmidt, A. E. Boehm, W. S. Tuten, A. Roorda, Spatial summation of individual cones in human color vision. *PLOS ONE* **14**, e0211397 (2019).

[Crossref](https://doi.org/10.1371/journal.pone.0211397)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/31344029/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000484977900002)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Spatial+summation+of+individual+cones+in+human+color+vision&author=B.+P.+Schmidt&author=A.+E.+Boehm&author=W.+S.+Tuten&author=A.+Roorda&publication_year=2019&journal=PLOS+ONE&pages=e0211397&doi=10.1371%2Fjournal.pone.0211397&pmid=31344029)

*   [a \[...\] ) or two](#body-ref-R8-1)
*   [b \[...\] that builds upon the cone-targeted methods](#body-ref-R8-2)

9

G. S. Brindley, The effects on colour vision of adaptation to very bright lights. *J. Physiol.* **122**, 332–350 (1953).

[Crossref](https://doi.org/10.1113/jphysiol.1953.sp005003)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/13118543/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+effects+on+colour+vision+of+adaptation+to+very+bright+lights&author=G.+S.+Brindley&publication_year=1953&journal=J.+Physiol.&pages=332-350&doi=10.1113%2Fjphysiol.1953.sp005003&pmid=13118543)

*   [a \[...\] red light before displaying green light](#body-ref-R9-1)
*   [b \[...\] so they are difficult to measure precisely](#body-ref-R9-2)

10

P. Churchland, Chimerical colors: Some phenomenological predictions from cognitive neuroscience. *Philos. Psychol.* **18**, 527–560 (2005).

[GO TO REFERENCE](#body-ref-R10)

[Crossref](https://doi.org/10.1080/09515080500264115)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Chimerical+colors%3A+Some+phenomenological+predictions+from+cognitive+neuroscience&author=P.+Churchland&publication_year=2005&journal=Philos.+Psychol.&pages=527-560&doi=10.1080%2F09515080500264115)

11

J. Koenderink, A. van Doorn, C. Witzel, K. Gegenfurtner, Hues of color afterimages. *i-Perception* **11**, 1–18 (2020).

[Crossref](https://doi.org/10.1177/2041669520903553)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Hues+of+color+afterimages&author=J.+Koenderink&author=A.+van+Doorn&author=C.+Witzel&author=K.+Gegenfurtner&publication_year=2020&journal=i-Perception&pages=1-18&doi=10.1177%2F2041669520903553)

*   [a \[...\] so they are difficult to measure precisely](#body-ref-R11-1)
*   [b \[...\] prior work in hue matching under fixation](#body-ref-R11-2)

12

O. Estévez, H. Spekreijse, The “silent substitution” method in visual research. *Vision Res.* **22**, 681–691 (1982).

[GO TO REFERENCE](#body-ref-R12)

[Crossref](https://doi.org/10.1016/0042-6989\(82\)90104-3)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/7112962/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+%E2%80%9Csilent+substitution%E2%80%9D+method+in+visual+research&author=O.+Est%C3%A9vez&author=H.+Spekreijse&publication_year=1982&journal=Vision+Res.&pages=681-691&doi=10.1016%2F0042-6989%2882%2990104-3&pmid=7112962)

13

R. L. De Valois, K. K. De Valois, E. Switkes, L. Mahon, Hue scaling of isoluminant and cone-specific lights. *Vision Res.* **37**, 885–897 (1997).

[GO TO REFERENCE](#body-ref-R13)

[Crossref](https://doi.org/10.1016/S0042-6989\(96\)00234-9)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/9156186/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1997WN64500007)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Hue+scaling+of+isoluminant+and+cone-specific+lights&author=R.+L.+De+Valois&author=K.+K.+De+Valois&author=E.+Switkes&author=L.+Mahon&publication_year=1997&journal=Vision+Res.&pages=885-897&doi=10.1016%2FS0042-6989%2896%2900234-9&pmid=9156186)

14

A. Roorda, F. Romero-Borja, W. J. Donnelly III, H. Queener, T. J. Hebert, M. C. W. Campbell, Adaptive optics scanning laser ophthalmoscopy. *Opt. Express* **10**, 405–412 (2002).

[Crossref](https://doi.org/10.1364/OE.10.000405)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/19436374/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000175459200003)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Adaptive+optics+scanning+laser+ophthalmoscopy&author=A.+Roorda&author=F.+Romero-Borja&author=W.+J.+Donnelly&author=H.+Queener&author=T.+J.+Hebert&author=M.+C.+W.+Campbell&publication_year=2002&journal=Opt.+Express&pages=405-412&doi=10.1364%2FOE.10.000405&pmid=19436374)

*   [a \[...\] scanning light ophthalmoscopy (AOSLO)](#body-ref-R14-1)
*   [b \[...\] We build our Oz prototype on an AOSLO](#body-ref-R14-2)
*   [c \[...\] platform described in previous publications](#body-ref-R14-3)

15

F. Zhang, K. Kurokawa, A. Lassoued, J. A. Crowell, D. T. Miller, Cone photoreceptor classification in the living human eye from photostimulation-induced phase dynamics. *Proc. Natl. Acad. Sci. U.S.A.* **116**, 7951–7956 (2019).

[Crossref](https://doi.org/10.1073/pnas.1816360116)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/30944223/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000464767500055)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Cone+photoreceptor+classification+in+the+living+human+eye+from+photostimulation-induced+phase+dynamics&author=F.+Zhang&author=K.+Kurokawa&author=A.+Lassoued&author=J.+A.+Crowell&author=D.+T.+Miller&publication_year=2019&journal=Proc.+Natl.+Acad.+Sci.+U.S.A.&pages=7951-7956&doi=10.1073%2Fpnas.1816360116&pmid=30944223)

*   [a \[...\] optical coherence tomography (AO-OCT)](#body-ref-R15-1)
*   [b \[...\] techniques in an AO-OCT system](#body-ref-R15-2)

16

V. P. Pandiyan, X. Jiang, A. Maloney-Bertelli, J. A. Kuchenbecker, U. Sharma, R. Sabesan, High-speed adaptive optics line-scan OCT for cellular-resolution optoretinography. *Biomed. Opt. Express* **11**, 5274–5296 (2020).

[GO TO REFERENCE](#body-ref-R16)

[Crossref](https://doi.org/10.1364/BOE.399034)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/33014614/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000577455500030)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=High-speed+adaptive+optics+line-scan+OCT+for+cellular-resolution+optoretinography&author=V.+P.+Pandiyan&author=X.+Jiang&author=A.+Maloney-Bertelli&author=J.+A.+Kuchenbecker&author=U.+Sharma&author=R.+Sabesan&publication_year=2020&journal=Biomed.+Opt.+Express&pages=5274-5296&doi=10.1364%2FBOE.399034&pmid=33014614)

17

A. Roorda, D. R. Williams, The arrangement of the three cone classes in the living human eye. *Nature* **397**, 520–522 (1999).

[GO TO REFERENCE](#body-ref-R17)

[Crossref](https://doi.org/10.1038/17383)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10028967/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000078574900049)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+arrangement+of+the+three+cone+classes+in+the+living+human+eye&author=A.+Roorda&author=D.+R.+Williams&publication_year=1999&journal=Nature&pages=520-522&doi=10.1038%2F17383&pmid=10028967)

18

D. W. Arathorn, Q. Yang, C. R. Vogel, Y. Zhang, P. Tiruveedhula, A. Roorda, Retinally stabilized cone-targeted stimulus delivery. *Opt. Express* **15**, 13731–13744 (2007).

[GO TO REFERENCE](#body-ref-R18)

[Crossref](https://doi.org/10.1364/OE.15.013731)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/19550644/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000251223100029)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Retinally+stabilized+cone-targeted+stimulus+delivery&author=D.+W.+Arathorn&author=Q.+Yang&author=C.+R.+Vogel&author=Y.+Zhang&author=P.+Tiruveedhula&author=A.+Roorda&publication_year=2007&journal=Opt.+Express&pages=13731-13744&doi=10.1364%2FOE.15.013731&pmid=19550644)

19

J. C. Maxwell, Experiments on colour, as perceived by the eye, with remarks on colour-blindness. *Earth Environ. Sci. Trans. R. Soc. Edinb.* **21**, 275–298 (1857).

[GO TO REFERENCE](#body-ref-R19)

[Crossref](https://doi.org/10.1017/S0080456800032117)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Experiments+on+colour%2C+as+perceived+by+the+eye%2C+with+remarks+on+colour-blindness&author=J.+C.+Maxwell&publication_year=1857&journal=Earth+Environ.+Sci.+Trans.+R.+Soc.+Edinb.&pages=275-298&doi=10.1017%2FS0080456800032117)

20

D. I. Macleod, D. R. Williams, W. Makous, A visual nonlinearity fed by single cones. *Vision Res.* **32**, 347–363 (1992).

[GO TO REFERENCE](#body-ref-R20)

[Crossref](https://doi.org/10.1016/0042-6989\(92\)90144-8)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/1574850/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=A+visual+nonlinearity+fed+by+single+cones&author=D.+I.+Macleod&author=D.+R.+Williams&author=W.+Makous&publication_year=1992&journal=Vision+Res.&pages=347-363&doi=10.1016%2F0042-6989%2892%2990144-8&pmid=1574850)

21

N. Sekiguchi, D. R. Williams, D. H. Brainard, Efficiency in detection of isoluminant and isochromatic interference fringes. *J. Opt. Soc. Am. A* **10**, 2118–2133 (1993).

[Crossref](https://doi.org/10.1364/JOSAA.10.002118)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1993LZ15700002)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Efficiency+in+detection+of+isoluminant+and+isochromatic+interference+fringes&author=N.+Sekiguchi&author=D.+R.+Williams&author=D.+H.+Brainard&publication_year=1993&journal=J.+Opt.+Soc.+Am.+A&pages=2118-2133&doi=10.1364%2FJOSAA.10.002118)

22

B. Chen, W. Makous, D. R. Williams, Serial spatial filters in vision. *Vision Res.* **33**, 413–427 (1993).

[Crossref](https://doi.org/10.1016/0042-6989\(93\)90095-E)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/8447111/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Serial+spatial+filters+in+vision&author=B.+Chen&author=W.+Makous&author=D.+R.+Williams&publication_year=1993&journal=Vision+Res.&pages=413-427&doi=10.1016%2F0042-6989%2893%2990095-E&pmid=8447111)

*   [a \[...\] the cone’s spatial light gathering function](#body-ref-R22-1)
*   [b \[...\] half the cone inner segment diameter (ISD)](#body-ref-R22-2)

23

V. P. Pandiyan, X. Jiang, J. A. Kuchenbecker, R. Sabesan, Reflective mirror-based line-scan adaptive optics OCT for imaging retinal structure and function. *Biomed. Opt. Express* **12**, 5865–5880 (2021).

[Crossref](https://doi.org/10.1364/BOE.436337)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/34692221/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000692527800005)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Reflective+mirror-based+line-scan+adaptive+optics+OCT+for+imaging+retinal+structure+and+function&author=V.+P.+Pandiyan&author=X.+Jiang&author=J.+A.+Kuchenbecker&author=R.+Sabesan&publication_year=2021&journal=Biomed.+Opt.+Express&pages=5865-5880&doi=10.1364%2FBOE.436337&pmid=34692221)

*   [a \[...\] techniques in an AO-OCT system](#body-ref-R23-1)
*   [b \[...\] not yet to the smallest cells in the fovea](#body-ref-R23-2)
*   [c \[...\] on a separate AO-OCT instrument](#body-ref-R23-3)

24

W. Kurtenbach, C. E. Sternheim, L. Spillmann, Change in hue of spectral colors by dilution with white light (Abney effect). *J. Opt. Soc. Am. A* **1**, 365–372 (1984).

[GO TO REFERENCE](#body-ref-R24)

[Crossref](https://doi.org/10.1364/JOSAA.1.000365)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/6726489/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Change+in+hue+of+spectral+colors+by+dilution+with+white+light+%28Abney+effect%29&author=W.+Kurtenbach&author=C.+E.+Sternheim&author=L.+Spillmann&publication_year=1984&journal=J.+Opt.+Soc.+Am.+A&pages=365-372&doi=10.1364%2FJOSAA.1.000365&pmid=6726489)

25

J. C. Maxwell, *On the Theory of Three Primary Colours* (Cambridge Univ. Press, 2011), pp. 445-450.

[GO TO REFERENCE](#body-ref-R25)

[Google Scholar](https://scholar.google.com/scholar?q=J.+C.+Maxwell%2C+On+the+Theory+of+Three+Primary+Colours+%28Cambridge+Univ.+Press%2C+2011%29%2C+pp.+445-450.)

26

V. P. Pandiyan, S. Schleufer, E. Slezak, J. Fong, R. Upadhyay, A. Roorda, R. Ng, R. Sabesan, Characterizing cone spectral classification by optoretinography. *Biomed. Opt. Express* **13**, 6574–6594 (2022).

[Crossref](https://doi.org/10.1364/BOE.473608)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/36589563/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000917262200028)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Characterizing+cone+spectral+classification+by+optoretinography&author=V.+P.+Pandiyan&author=S.+Schleufer&author=E.+Slezak&author=J.+Fong&author=R.+Upadhyay&author=A.+Roorda&author=R.+Ng&author=R.+Sabesan&publication_year=2022&journal=Biomed.+Opt.+Express&pages=6574-6594&doi=10.1364%2FBOE.473608&pmid=36589563)

*   [a \[...\] not yet to the smallest cells in the fovea](#body-ref-R26-1)
*   [b \[...\] the Retina Map Alignment Tool \[described in](#body-ref-R26-2)

27

J. E. Vanston, A. E. Boehm, W. S. Tuten, A. Roorda, It’s not easy seeing green: The veridical perception of small spots. *J. Vis.* **23**, 2 (2023).

[GO TO REFERENCE](#body-ref-R27)

[Crossref](https://doi.org/10.1167/jov.23.5.2)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/37133838/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000994047800008)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=It%E2%80%99s+not+easy+seeing+green%3A+The+veridical+perception+of+small+spots&author=J.+E.+Vanston&author=A.+E.+Boehm&author=W.+S.+Tuten&author=A.+Roorda&publication_year=2023&journal=J.+Vis.&pages=2&doi=10.1167%2Fjov.23.5.2&pmid=37133838)

28

M. J. Greene, A. E. Boehm, J. E. Vanston, V. P. Pandiyan, R. Sabesan, W. S. Tuten, Unique yellow shifts for small and brief stimuli in the central retina. *J. Vis.* **24**, 2 (2024).

[GO TO REFERENCE](#body-ref-R28)

[Crossref](https://doi.org/10.1167/jov.24.6.2)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/38833255/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A001244308500002)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Unique+yellow+shifts+for+small+and+brief+stimuli+in+the+central+retina&author=M.+J.+Greene&author=A.+E.+Boehm&author=J.+E.+Vanston&author=V.+P.+Pandiyan&author=R.+Sabesan&author=W.+S.+Tuten&publication_year=2024&journal=J.+Vis.&pages=2&doi=10.1167%2Fjov.24.6.2&pmid=38833255)

29

J. Freeman, G. D. Field, P. H. Li, M. Greschner, D. E. Gunning, K. Mathieson, A. Sher, A. M. Litke, L. Paninski, E. P. Simoncelli, E. J. Chichilnisky, Mapping nonlinear receptive field structure in primate retina at single cone resolution. *eLife* **4**, e05241 (2015).

[GO TO REFERENCE](#body-ref-R29)

[Crossref](https://doi.org/10.7554/eLife.05241)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/26517879/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000373884300001)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Mapping+nonlinear+receptive+field+structure+in+primate+retina+at+single+cone+resolution&author=J.+Freeman&author=G.+D.+Field&author=P.+H.+Li&author=M.+Greschner&author=D.+E.+Gunning&author=K.+Mathieson&author=A.+Sher&author=A.+M.+Litke&author=L.+Paninski&author=E.+P.+Simoncelli&author=E.+J.+Chichilnisky&publication_year=2015&journal=eLife&pages=e05241&doi=10.7554%2FeLife.05241&pmid=26517879)

30

J. B. Demb, L. Haarsma, M. A. Freed, P. Sterling, Functional circuitry of the retinal ganglion cell’s nonlinear receptive field. *J. Neurosci.* **19**, 9756–9767 (1999).

[GO TO REFERENCE](#body-ref-R30)

[Crossref](https://doi.org/10.1523/JNEUROSCI.19-22-09756.1999)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/10559385/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Functional+circuitry+of+the+retinal+ganglion+cell%E2%80%99s+nonlinear+receptive+field&author=J.+B.+Demb&author=L.+Haarsma&author=M.+A.+Freed&author=P.+Sterling&publication_year=1999&journal=J.+Neurosci.&pages=9756-9767&doi=10.1523%2FJNEUROSCI.19-22-09756.1999&pmid=10559385)

31

D. R. Coates, X. Jiang, J. A. Kuchenbecker, R. Sabesan, The Rayleigh limit of the parvocellular pathway. *Invest. Ophthalmol. Vis. Sci.* **60**, 1312 (2019).

[GO TO REFERENCE](#body-ref-R31)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000488628103093)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+Rayleigh+limit+of+the+parvocellular+pathway&author=D.+R.+Coates&author=X.+Jiang&author=J.+A.+Kuchenbecker&author=R.+Sabesan&publication_year=2019&journal=Invest.+Ophthalmol.+Vis.+Sci.&pages=1312)

32

A. M. Geller, P. A. Sieving, D. G. Green, Effect on grating identification of sampling with degenerate arrays. *J. Opt. Soc. Am. A* **9**, 472–477 (1992).

[GO TO REFERENCE](#body-ref-R32)

[Crossref](https://doi.org/10.1364/JOSAA.9.000472)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/1548555/)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Effect+on+grating+identification+of+sampling+with+degenerate+arrays&author=A.+M.+Geller&author=P.+A.+Sieving&author=D.+G.+Green&publication_year=1992&journal=J.+Opt.+Soc.+Am.+A&pages=472-477&doi=10.1364%2FJOSAA.9.000472&pmid=1548555)

33

K. Ratnam, J. Carroll, T. C. Porco, J. L. Duncan, A. Roorda, Relationship between foveal cone structure and clinical measures of visual function in patients with inherited retinal degenerations. *Invest. Ophthalmol. Vis. Sci.* **54**, 5836–5847 (2013).

[GO TO REFERENCE](#body-ref-R33)

[Crossref](https://doi.org/10.1167/iovs.13-12557)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/23908179/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000325167200086)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Relationship+between+foveal+cone+structure+and+clinical+measures+of+visual+function+in+patients+with+inherited+retinal+degenerations&author=K.+Ratnam&author=J.+Carroll&author=T.+C.+Porco&author=J.+L.+Duncan&author=A.+Roorda&publication_year=2013&journal=Invest.+Ophthalmol.+Vis.+Sci.&pages=5836-5847&doi=10.1167%2Fiovs.13-12557&pmid=23908179)

34

K. Mancuso, W. W. Hauswirth, Q. Li, T. B. Connor, J. A. Kuchenbecker, M. C. Mauck, J. Neitz, M. Neitz, Gene therapy for red–green colour blindness in adult primates. *Nature* **461**, 784–787 (2009).

[GO TO REFERENCE](#body-ref-R34)

[Crossref](https://doi.org/10.1038/nature08401)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/19759534/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000270547500034)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Gene+therapy+for+red%E2%80%93green+colour+blindness+in+adult+primates&author=K.+Mancuso&author=W.+W.+Hauswirth&author=Q.+Li&author=T.+B.+Connor&author=J.+A.+Kuchenbecker&author=M.+C.+Mauck&author=J.+Neitz&author=M.+Neitz&publication_year=2009&journal=Nature&pages=784-787&doi=10.1038%2Fnature08401&pmid=19759534)

35

G. H. Jacobs, Photopigments and the dimensionality of animal color vision. *Neurosci. Biobehav. Rev.* **86**, 108–130 (2018).

[GO TO REFERENCE](#body-ref-R35)

[Crossref](https://doi.org/10.1016/j.neubiorev.2017.12.006)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/29224775/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000426224600010)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Photopigments+and+the+dimensionality+of+animal+color+vision&author=G.+H.+Jacobs&publication_year=2018&journal=Neurosci.+Biobehav.+Rev.&pages=108-130&doi=10.1016%2Fj.neubiorev.2017.12.006&pmid=29224775)

36

C. A. Curcio, K. R. Sloan, R. E. Kalina, A. E. Hendrickson, Human photoreceptor topography. *J. Comp. Neurol.* **292**, 497–523 (1990).

[Crossref](https://doi.org/10.1002/cne.902920402)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/2324310/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1990CR16200001)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Human+photoreceptor+topography&author=C.+A.+Curcio&author=K.+R.+Sloan&author=R.+E.+Kalina&author=A.+E.+Hendrickson&publication_year=1990&journal=J.+Comp.+Neurol.&pages=497-523&doi=10.1002%2Fcne.902920402&pmid=2324310)

*   [a \[...\] because of increasing spacing of cones](#body-ref-R36-1)
*   [b \[...\] respectively, consistent with Curcio et al.](#body-ref-R36-2)

37

W. M. Harmening, W. S. Tuten, A. Roorda, L. C. Sincich, Mapping the perceptual grain of the human retina. *J. Neurosci.* **34**, 5667–5677 (2014).

[GO TO REFERENCE](#body-ref-R37)

[Crossref](https://doi.org/10.1523/JNEUROSCI.5191-13.2014)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/24741057/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000334926000028)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Mapping+the+perceptual+grain+of+the+human+retina&author=W.+M.+Harmening&author=W.+S.+Tuten&author=A.+Roorda&author=L.+C.+Sincich&publication_year=2014&journal=J.+Neurosci.&pages=5667-5677&doi=10.1523%2FJNEUROSCI.5191-13.2014&pmid=24741057)

38

D. A. Atchison, G. Smith, Chromatic dispersions of the ocular media of human eyes. *J. Opt. Soc. Am. A* **22**, 29–37 (2005).

[GO TO REFERENCE](#body-ref-R38)

[Crossref](https://doi.org/10.1364/JOSAA.22.000029)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000226397300003)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Chromatic+dispersions+of+the+ocular+media+of+human+eyes&author=D.+A.+Atchison&author=G.+Smith&publication_year=2005&journal=J.+Opt.+Soc.+Am.+A&pages=29-37&doi=10.1364%2FJOSAA.22.000029)

39

J. Liang, D. R. Williams, D. T. Miller, Supernormal vision and high-resolution retinal imaging through adaptive optics. *J. Opt. Soc. Am. A* **14**, 2884–2892 (1997).

[GO TO REFERENCE](#body-ref-R39)

[Crossref](https://doi.org/10.1364/JOSAA.14.002884)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Supernormal+vision+and+high-resolution+retinal+imaging+through+adaptive+optics&author=J.+Liang&author=D.+R.+Williams&author=D.+T.+Miller&publication_year=1997&journal=J.+Opt.+Soc.+Am.+A&pages=2884-2892&doi=10.1364%2FJOSAA.14.002884)

40

Q. Yang, D. W. Arathorn, P. Tiruveedhula, C. R. Vogel, A. Roorda, Design of an integrated hardware interface for AOSLO image capture and cone-targeted stimulus delivery. *Opt. Express* **18**, 17841–17858 (2010).

[GO TO REFERENCE](#body-ref-R40)

[Crossref](https://doi.org/10.1364/OE.18.017841)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/20721171/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000281054400028)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Design+of+an+integrated+hardware+interface+for+AOSLO+image+capture+and+cone-targeted+stimulus+delivery&author=Q.+Yang&author=D.+W.+Arathorn&author=P.+Tiruveedhula&author=C.+R.+Vogel&author=A.+Roorda&publication_year=2010&journal=Opt.+Express&pages=17841-17858&doi=10.1364%2FOE.18.017841&pmid=20721171)

41

A. E. Boehm, C. M. Privitera, B. P. Schmidt, A. Roorda, Transverse chromatic offsets with pupil displacements in the human eye: Sources of variability and methods for real-time correction. *Biomed. Opt. Express* **10**, 1691–1706 (2019).

[GO TO REFERENCE](#body-ref-R41)

[Crossref](https://doi.org/10.1364/BOE.10.001691)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/31061763/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000462887400013)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Transverse+chromatic+offsets+with+pupil+displacements+in+the+human+eye%3A+Sources+of+variability+and+methods+for+real-time+correction&author=A.+E.+Boehm&author=C.+M.+Privitera&author=B.+P.+Schmidt&author=A.+Roorda&publication_year=2019&journal=Biomed.+Opt.+Express&pages=1691-1706&doi=10.1364%2FBOE.10.001691&pmid=31061763)

42

J. Shenoy, J. Fong, J. Tan, A. Roorda, R. Ng, “R-SLAM: Optimizing eye tracking from rolling shutter video of the retina,” in *Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision* (IEEE, 2021), pp. 4852–4861.

[Google Scholar](https://scholar.google.com/scholar?q=J.+Shenoy%2C+J.+Fong%2C+J.+Tan%2C+A.+Roorda%2C+R.+Ng%2C+%E2%80%9CR-SLAM%3A+Optimizing+eye+tracking+from+rolling+shutter+video+of+the+retina%2C%E2%80%9D+in+Proceedings+of+the+2021+IEEE%2FCVF+International+Conference+on+Computer+Vision+%28IEEE%2C+2021%29%2C+pp.+4852%E2%80%934861.)

*   [a \[...\] a global optimization algorithm, R-SLAM](#body-ref-R42-1)
*   [b \[...\] is to modify the R-SLAM algorithm](#body-ref-R42-2)

43

S. B. Stevenson, A. Roorda, “Correcting for miniature eye movements in high resolution scanning laser ophthalmoscopy” in *SPIE Proceedings 5688* (SPIE, 2005), pp. 145–151.

[Google Scholar](https://scholar.google.com/scholar?q=S.+B.+Stevenson%2C+A.+Roorda%2C+%E2%80%9CCorrecting+for+miniature+eye+movements+in+high+resolution+scanning+laser+ophthalmoscopy%E2%80%9D+in+SPIE+Proceedings+5688+%28SPIE%2C+2005%29%2C+pp.+145%E2%80%93151.)

*   [a \[...\] (NCC) strip-based matching method](#body-ref-R43-1)
*   [b \[...\] of the independently tracked halves](#body-ref-R43-2)

44

M. A. Fischler, R. C. Bolles, Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. *Commun. ACM* **24**, 381–395 (1981).

[GO TO REFERENCE](#body-ref-R44)

[Crossref](https://doi.org/10.1145/358669.358692)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Random+sample+consensus%3A+A+paradigm+for+model+fitting+with+applications+to+image+analysis+and+automated+cartography&author=M.+A.+Fischler&author=R.+C.+Bolles&publication_year=1981&journal=Commun.+ACM&pages=381-395&doi=10.1145%2F358669.358692)

45

J. Fong, “How to See Impossible Colors: First Steps Toward the Oz Vision Display,” thesis, University of California, Berkeley (2021).

[GO TO REFERENCE](#body-ref-R45)

[Google Scholar](https://scholar.google.com/scholar?q=J.+Fong%2C+%E2%80%9CHow+to+See+Impossible+Colors%3A+First+Steps+Toward+the+Oz+Vision+Display%2C%E2%80%9D+thesis%2C+University+of+California%2C+Berkeley+%282021%29.)

46

W. M. Harmening, P. Tiruveedhula, A. Roorda, L. C. Sincich, Measurement and correction of transverse chromatic offsets for multi-wavelength retinal microscopy in the living eye. *Biomed. Opt. Express* **3**, 2066–2077 (2012).

[Crossref](https://doi.org/10.1364/BOE.3.002066)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/23024901/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000308861100010)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Measurement+and+correction+of+transverse+chromatic+offsets+for+multi-wavelength+retinal+microscopy+in+the+living+eye&author=W.+M.+Harmening&author=P.+Tiruveedhula&author=A.+Roorda&author=L.+C.+Sincich&publication_year=2012&journal=Biomed.+Opt.+Express&pages=2066-2077&doi=10.1364%2FBOE.3.002066&pmid=23024901)

*   [a \[...\] improve on previous image-based procedures](#body-ref-R46-1)
*   [b \[...\] TCA is highly dependent on pupil location](#body-ref-R46-2)

47

D. H. Brainard, The psychophysics toolbox. *Spat. Vis.* **10**, 433–436 (1997).

[GO TO REFERENCE](#body-ref-R47)

[Crossref](https://doi.org/10.1163/156856897X00357)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/9176952/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1997WZ53600014)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+psychophysics+toolbox&author=D.+H.+Brainard&publication_year=1997&journal=Spat.+Vis.&pages=433-436&doi=10.1163%2F156856897X00357&pmid=9176952)

48

D. G. Pelli, The videotoolbox software for visual psychophysics: Transforming numbers into movies. *Spat. Vis.* **10**, 437–442 (1997).

[Crossref](https://doi.org/10.1163/156856897X00366)

[PubMed](https://pubmed.ncbi.nlm.nih.gov/9176953/)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3AA1997WZ53600015)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+videotoolbox+software+for+visual+psychophysics%3A+Transforming+numbers+into+movies&author=D.+G.+Pelli&publication_year=1997&journal=Spat.+Vis.&pages=437-442&doi=10.1163%2F156856897X00366&pmid=9176953)

49

M. Kleiner, D. Brainard, D. Pelli, A. Ingling, R. Murray, C. Broussard, What’s new in psychtoolbox-3. *Perception* **36**, 1–16 (2007).

[GO TO REFERENCE](#body-ref-R49)

[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=WOS%3A000250594600049)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=What%E2%80%99s+new+in+psychtoolbox-3&author=M.+Kleiner&author=D.+Brainard&author=D.+Pelli&author=A.+Ingling&author=R.+Murray&author=C.+Broussard&publication_year=2007&journal=Perception&pages=1-16)

50

“Amendment 1 - Multimedia systems and equipment - Colour measurement and management - Part 2-1: Colour management - Default RGB colour space – sRGB” (Tech. Rep. IEC 61966-2- 1:1999/AMD1:2003, International Electrotechnical Commission, 2003).

[GO TO REFERENCE](#body-ref-R50)

[Google Scholar](https://scholar.google.com/scholar?q=%E2%80%9CAmendment+1+-+Multimedia+systems+and+equipment+-+Colour+measurement+and+management+-+Part+2-1%3A+Colour+management+-+Default+RGB+colour+space+%E2%80%93+sRGB%E2%80%9D+%28Tech.+Rep.+IEC+61966-2-+1%3A1999%2FAMD1%3A2003%2C+International+Electrotechnical+Commission%2C+2003%29.)

51

“Colorimetry — Part 4: CIE 1976 l\*a\*b\* colour space” (Tech. Rep. ISO/CIE 11664-4:2019, International Commission on Illumination, 2019).

[Google Scholar](https://scholar.google.com/scholar?q=%E2%80%9CColorimetry+%E2%80%94+Part+4%3A+CIE+1976+l%2Aa%2Ab%2A+colour+space%E2%80%9D+%28Tech.+Rep.+ISO%2FCIE+11664-4%3A2019%2C+International+Commission+on+Illumination%2C+2019%29.)

*   [a \[...\] ) colors with matching CIELAB hue](#body-ref-R51-1)
*   [b \[...\] in standards such as CIELAB and ΔEab\*](#body-ref-R51-2)

52

D. L. MacAdam, Visual sensitivities to color differences in daylight. *J. Opt. Soc. Am.* **32**, 247–274 (1942).

[GO TO REFERENCE](#body-ref-R52)

[Crossref](https://doi.org/10.1364/JOSA.32.000247)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Visual+sensitivities+to+color+differences+in+daylight&author=D.+L.+MacAdam&publication_year=1942&journal=J.+Opt.+Soc.+Am.&pages=247-274&doi=10.1364%2FJOSA.32.000247)

53

M. Mahy, L. Van Eycken, A. Oosterlinck, Evaluation of uniform color spaces developed after the adoption of CIELAB and CIELUV. *Color Res. Appl.* **19**, 105–121 (1994).

[GO TO REFERENCE](#body-ref-R53)

[Crossref](https://doi.org/10.1111/j.1520-6378.1994.tb00070.x)

[Google Scholar](https://scholar.google.com/scholar_lookup?title=Evaluation+of+uniform+color+spaces+developed+after+the+adoption+of+CIELAB+and+CIELUV&author=M.+Mahy&author=L.+Van+Eycken&author=A.+Oosterlinck&publication_year=1994&journal=Color+Res.+Appl.&pages=105-121&doi=10.1111%2Fj.1520-6378.1994.tb00070.x)

54

W. M. Harmening, L. C. Sincich, in *Adaptive Optics for Photoreceptor-Targeted Psychophysics*, (Springer International Publishing, Cham, 2019), pp. 359–375.

[GO TO REFERENCE](#body-ref-R54)

[Google Scholar](https://scholar.google.com/scholar?q=W.+M.+Harmening%2C+L.+C.+Sincich%2C+in+Adaptive+Optics+for+Photoreceptor-Targeted+Psychophysics%2C+%28Springer+International+Publishing%2C+Cham%2C+2019%29%2C+pp.+359%E2%80%93375.)

#### Current Issue

[![Science Advances cover image](/cdn-cgi/image/width=400/cms/asset/731c4cd6-8da9-460f-9b34-7fd3893173b5/sciadv.2025.11.issue-16.largecover.jpg)](/toc/sciadv/11/16 "View Science Advances current issue")

### [Navigating and attributing uncertainty in future tropical cyclone risk estimates](/doi/10.1126/sciadv.adn4607 "Navigating and attributing uncertainty in future tropical cyclone risk estimates")

*   By
    *   Simona Meiler
    *   Chahan M. Kropf
    *   *et al.*

### [Targeting PGM3 abolishes SREBP-1 activation-hexosamine synthesis feedback regulation to effectively suppress brain tumor growth](/doi/10.1126/sciadv.adq0334 "Targeting PGM3 abolishes SREBP-1 activation-hexosamine synthesis feedback regulation to effectively suppress brain tumor growth")

*   By
    *   Huali Su
    *   Yaogang Zhong
    *   *et al.*

### [Broadband localization of light at the termination of a topological photonic waveguide](/doi/10.1126/sciadv.adr9569 "Broadband localization of light at the termination of a topological photonic waveguide")

*   By
    *   Daniel Muis
    *   Yandong Li
    *   *et al.*

[Table of Contents](/toc/sciadv/current)

#### Advertisement

### Sign up for ScienceAdviser

Get Science’s award-winning newsletter with the latest news, commentary, and research, free to your inbox daily.

[Subscribe](/content/page/scienceadviser?intcmp=rrail-adviser&utm_id=recFUzjFNRznSEEDd)

#### LATEST NEWS

[ScienceInsider](/news/scienceinsider)21 Apr 2025

[New NIH director defends grant cuts as part of shift to support MAHA vision](/content/article/new-nih-director-defends-grant-cuts-part-shift-support-maha-vision "New NIH director defends grant cuts as part of shift to support MAHA vision")

[News](/news/all-news)21 Apr 2025

[Ultracool microscopy yields a sharper look at proteins](/content/article/ultracool-microscopy-yields-sharper-look-proteins "Ultracool microscopy yields a sharper look at proteins")

[ScienceInsider](/news/scienceinsider)21 Apr. 2025

[EPA orders staff to begin canceling research grants](/content/article/epa-orders-staff-begin-canceling-research-grants "EPA orders staff to begin canceling research grants")

[ScienceInsider](/news/scienceinsider)18 Apr 2025

[NSF starts to kill grants that violate Trump’s war on diversity efforts](/content/article/nsf-starts-kill-grants-violate-trump-s-war-diversity-efforts "NSF starts to kill grants that violate Trump’s war on diversity efforts")

[ScienceInsider](/news/scienceinsider)18 Apr 2025

[Trump swings budget ax at USGS biology research](/content/article/trump-swings-budget-ax-usgs-biology-research "Trump swings budget ax at USGS biology research")

[ScienceInsider](/news/scienceinsider)18 Apr 2025

[NIH halts more collaborations with South Africa on HIV/AIDS trials](/content/article/nih-halts-more-collaborations-south-africa-hiv-aids-trials "NIH halts more collaborations with South Africa on HIV/AIDS trials")

#### Advertisement

## Recommended[Close](#)

ReportFebruary 1977

[Rod Photoreceptors Detect Rapid Flicker](/doi/full/10.1126/science.841308 "Rod Photoreceptors Detect Rapid Flicker")

ReportMay 1981

[Color and Luminance: Independent Frequency Shifts](/doi/full/10.1126/science.7221569 "Color and Luminance: Independent Frequency Shifts")

Technical CommentsOctober 2007

[Response to Comment on "Emergence of Novel Color Vision in Mice Engineered to Express a Human Cone Photopigment"](/doi/full/10.1126/science.1146519 "Response to Comment on \"Emergence of Novel Color Vision in Mice Engineered to Express a Human Cone Photopigment\"")

ReportsApril 1999

[Regulation of Mammalian Circadian Behavior by Non-rod, Non-cone, Ocular Photoreceptors](/doi/full/10.1126/science.284.5413.502 "Regulation of Mammalian Circadian Behavior by Non-rod, Non-cone, Ocular Photoreceptors")

SPONSORED [webinar](/custom-publishing/webinars "webinars")

Science and Life 8 May 2025

[Redefining Connection: How young people are shaping the future of the rare disease community through technology and innovation](/content/webinar/redefining-connection-how-young-people-are-shaping-future-rare-disease-community "Redefining Connection: How young people are shaping the future of the rare disease community through technology and innovation")

#### Advertisement

[View full text](/doi/full/10.1126/sciadv.adu1052)|[Download PDF](/doi/pdf/10.1126/sciadv.adu1052)

[Home](https://www.science.org/)[Science Advances](/journal/sciadv)[Vol. 11, No. 16](/toc/sciadv/11/16)Novel color via stimulation of individual photoreceptors at population scale

[Back To Vol. 11, No. 16](/toc/sciadv/11/16)

[SHARE](#)

*   [](# "Share on Facebook")
*   [](# "Share on X (formerly Twitter)")
*   [](# "Share on LinkedIn")
*   [](# "Share on Reddit")
*   [](# "Share on WeChat")
*   [](# "Share on WhatsApp")
*   [](# "Share on Bluesky")
*   [](# "Share on email")

[Notifications](/action/addCitationAlert?doi=10.1126%2Fsciadv.adu1052)

[Bookmark](/personalize/addFavoritePublication?doi=10.1126%2Fsciadv.adu1052)

[](/doi/reader/10.1126/sciadv.adu1052 "PDF")

[

###### PREVIOUS ARTICLE

Hippocampal perineuronal net degradation identifies prefrontal and striatal circuits involved in schizophrenia-like changes in marmosets

Previous](/doi/10.1126/sciadv.adu0975)[

###### NEXT ARTICLE

Earlier onset of chemotherapy-induced neuropathic pain in females by ICAM-1–mediated accumulation of perivascular macrophages

Next](/doi/10.1126/sciadv.adu2159)

FiguresTables

[](# "Close figure viewer")

[![](/cms/10.1126/sciadv.adu1052/asset/676ebf10-6471-4c5a-9b79-dfe102d331a1/assets/images/large/sciadv.adu1052-f1.jpg)View figure](#fv-F1)

Fig. 1

Fig. 1. Overview of principle and prototype system.

(**A**) System inputs. (i) Retina map of 103 cone cells preclassified by spectral type ([*7*](#core-R7)). (ii) Target visual percept (here, a video of a child, see movie S1 at 1:04). (iii) Infrared cellular-scale imaging of the retina with 60-frames-per-second rolling shutter. Fixational eye movement is visible over the three frames shown. (**B**) System outputs. (iv) Real-time per-cone target activation levels to reproduce the target percept, computed by: extracting eye motion from the input video relative to the retina map; identifying the spectral type of every cone in the field of view; computing the per-cone activation the target percept would have produced. (v) Intensities of visible-wavelength 488-nm laser microdoses at each cone required to achieve its target activation level. (**C**) Infrared imaging and visible-wavelength stimulation are physically accomplished in a raster scan across the retinal region using AOSLO. By modulating the visible-wavelength beam’s intensity, the laser microdoses shown in (v) are delivered. Drawing adapted with permission \[Harmening and Sincich ([*54*](#core-R54))\]. (**D**) Examples of target percepts with corresponding cone activations and laser microdoses, ranging from colored squares to complex imagery. Teal-striped regions represent the color “olo” of stimulating only M cones.

[![](/cms/10.1126/sciadv.adu1052/asset/d0883609-d37f-450f-b297-5a934fddff67/assets/images/large/sciadv.adu1052-f2.jpg)View figure](#fv-F2)

Fig. 2

Fig. 2. Theoretical model of Oz color gamut as a function of fractional leak and stimulation wavelength.

(**A**) Gamut shrinks from the full *lms* chromaticity triangle to the stimulation wavelength (open circle) as the fractional light leak grows; note that this fraction depends on the intercone spacing, which varies across the retina. The colored region is the gamut of natural human colors. (**B**) Gamut varies in chromaticity, position, and shape as a function of stimulation wavelength. For readability, extra copies of the gamuts for 543 and 589 nm are drawn next to the *lm* edge.

[![](/cms/10.1126/sciadv.adu1052/asset/fb500790-9651-44e9-998b-9df4d8cd125c/assets/images/large/sciadv.adu1052-f3.jpg)View figure](#fv-F3)

Fig. 3

Fig. 3. Color matching of Oz colored squares produced by cone-by-cone stimulation.

(**A** to **D**) Each *lms* chromaticity triangle plots color matches for one subject with the indicated stimulation wavelength and type of matching color system (RGB projector, or tunable near-monochromatic laser and projector white). Target colors are specified as (L, M, and S) triplets, which are the relative light intensity levels directed to each cone class. Color matches to different target colors are denoted with differently colored markers. Each triangle also plots: color matches for randomly interleaved jitter control condition \[see (E) and the “Design of prototype” section\]; coordinates of the stimulation wavelength; natural color gamut of human vision; gamut of the matching color system and its whitepoint; and perceptual uncertainty ellipses for the average color matches (projected JND ellipsoid at the coordinates of the “positive” component of the color match, computed from CIELAB/ΔEab\*, scaled three times the actual size; see the “Plotting perceptual uncertainty in matching” section in Materials and Methods). Ellipses not visible are smaller than their associated markers. (**E**) Illustration of the control condition randomly interleaved into all experiments: microdose target locations are randomly jittered by two intercone spacings in Oz stimuli that are otherwise identical to the experimental condition.

[![](/cms/10.1126/sciadv.adu1052/asset/570cf67c-7982-4046-9ca3-0b9d0965b71f/assets/images/large/sciadv.adu1052-f4.jpg)View figure](#fv-F4)

Fig. 4

Fig. 4. Image and video recognition experiments.

We tested subjects’ ability to recognize image and video content consisting of Oz colors: (**A**) a 4-alternative forced choice (4-AFC) line orientation recognition task, and (**B**), a 2-AFC rotation direction task experiments. Oz stimuli consisted of equiluminant red lines and disks presented on an olo background, as depicted. The bar graphs show individual subject performance over 20 trials per condition and average accuracy across five subjects with 95% confidence intervals. In experimental conditions with Oz microdoses delivered experimentally (blue bars), subjects are able to accurately identify line orientation and rotation direction. In control-group stimuli (gray bars), where cone-targeting is compromised through jittering microdose target locations, task accuracy is reduced to guessing rate as indicated by the dashed lines.

[![](/cms/10.1126/sciadv.adu1052/asset/c22483c4-b38d-46d4-a57a-69f6ce368513/assets/images/large/sciadv.adu1052-f5.jpg)View figure](#fv-F5)

Fig. 5

Fig. 5. Subject’s view during color matching experiment.

Left shows the experimental view. Right shows an example of the multicolored mosaics shown for a periodic 15-s “refresh period.”

[![](/cms/10.1126/sciadv.adu1052/asset/fa2481cf-1382-40a7-b61c-0188477622b2/assets/images/large/sciadv.adu1052-f6.jpg)View figure](#fv-F6)

Fig. 6

Fig. 6. Computation of perceptual uncertainty ellipses.

(**A**) Human perceptual JND ellipsoids \[e.g., *E*(*c*) at color *c*\] are long and skinny, pointing at the origin in LMS Cartesian space. They project to ellipses in the (l,m,s)\=(L,M,S)L+M+S chromaticity triangle as shown. (**B**) Vector math for computing the coordinates of a color match *c**m* = *c**p* − *c**n*, where *c**p* is the “positive” color seen by subject at matching, and *c**n* is the “negative” light added to the test color to enable a match. (**C**) In color matching, because the color seen by the subject at the time of match submission is the “positive” color *c**p*, the perceptual uncertainty of the inferred color match *c**m* is the ellipsoid *E*(*c**p*), recentered on *c**m*, as shown. If *c**n* is non-negative, as shown, the ellipsoid recentered on *c**m*, *E*(*c**p*) − *c**n*, no longer points at the origin, and projects to a nonlinearly enlarged ellipse in the *lms* triangle. Therefore, it is desirable to minimize the “negative” light required to achieve a color match, as accomplished with the tunable monochromatic light source used for matching olo (see [Fig. 3](#F3)).

Reference #1

[](# "close pop-up")

[Skip slideshow](#afterSlideshow-j3n)

[](/journal/science "Science Journal Logo")

[](/journal/sciadv "Science Advances Journal Logo")

[](/journal/sciimmunol "Science Immunology Journal Logo")

[](/journal/scirobotics "Science Robotics Journal Logo")

[](/journal/signaling "Science Signaling Journal Logo")

[](/journal/stm "Science Translational Medicine Journal Logo")

###### [Follow Us](https://www.science.org/content/page/science-family-journals-social-media)

*   [](https://www.facebook.com/ScienceMagazine "Facebook")
*   [](https://twitter.com/sciencemagazine "Share on X")
*   [](https://www.instagram.com/ScienceMagazine "Instagram")
*   [](https://www.youtube.com/user/ScienceMag "Youtube")
*   [](/about/email-alerts-and-rss-feeds "RSS feeds")
*   [](https://mp.weixin.qq.com/s?__biz=MzI3NDY3NzQ2Mg==&mid=100002815&idx=1&sn=2949c025a553ac718b9612a0473b9f60&chksm=6b1120465c66a9508b01eaef1589b15d440e50b189106c8c594de8c6471f696a978de952fb15&mpshare=1&scene=1&srcid=0716JJQ5V4cKbgMMsya2MQ0n&sharer_sharetime= "WeChat")
*   [Get our newsletter](/content/page/scienceadviser?intcmp=ftr-adviser&utm_id=recdExfxt1yeSJxzi)

*   [NEWS](/news)
*   [All News](/news/all-news)
*   [ScienceInsider](/news/scienceinsider)
*   [News Features](/news/features)
*   [Subscribe to News from Science](/content/page/news-science-subscriptions?intcmp=footer-subscribetonews&utm_id=recziGpGO7jMVeez2)
*   [News from Science FAQ](/content/page/news-subscriber-faqs)
*   [About News from Science](/content/page/about-news-science)
*   [Donate to News](/news/donate?intcmp=footer-donate&utm_id=recVQVKKRRLdidlGT)

*   [CAREERS](/careers)
*   [Careers Articles](/topic/article-type/careers-editorial)
*   [Find Jobs](https://jobs.sciencecareers.org/)
*   [Employer Hubs](/careers/employers)

*   [COMMENTARY](/commentary)
*   [Opinion](/commentary/opinion)
*   [Analysis](/commentary/analysis)
*   [Blogs](/blogs)

*   [JOURNALS](/journals)
*   [Science](/journal/science)
*   [Science Advances](/journal/sciadv)
*   [Science Immunology](/journal/sciimmunol)
*   [Science Robotics](/journal/scirobotics)
*   [Science Signaling](/journal/signaling)
*   [Science Translational Medicine](/journal/stm)
*   [Science Partner Journals](https://spj.sciencemag.org/)

*   [AUTHORS & REVIEWERS](/content/page/contributing-science-family-journals)
*   [Information for Authors](/content/page/contributing-science-family-journals)
*   [Information for Reviewers](/content/page/peer-review-science-publications)

*   [LIBRARIANS](/content/page/librarian-portal)
*   [Manage Your Institutional Subscription](/action/institutionAccessEntitlements)
*   [Library Admin Portal](/content/page/librarian-portal)
*   [Request a Quote](https://scienceaaas.org/request)
*   [Librarian FAQs](/content/page/librarian-portal-frequently-asked-questions)

*   [ADVERTISERS](https://advertising.sciencemag.org/)
*   [Advertising Kits](https://advertising.sciencemag.org/)
*   [Custom Publishing Info](/custom-publishing)
*   [Post a Job](https://employers.sciencecareers.org/)

*   [RELATED SITES](/content/page/related-sites)
*   [AAAS.org](//www.aaas.org/)
*   [AAAS Communities](//members.aaas.org/home)
*   [EurekAlert!](//www.eurekalert.org/)
*   [Science in the Classroom](//www.scienceintheclassroom.org/)

*   [ABOUT US](/content/page/aboutus)
*   [Leadership](/content/page/leadership-and-management)
*   [Work at AAAS](//www.aaas.org/careers/workataaas)
*   [Prizes and Awards](/content/page/prizes-and-awards)

*   [HELP](/content/page/help)
*   [FAQs](/content/page/faqs)
*   [Access and Subscriptions](/content/page/access-and-subscriptions)
*   [Order a Single Issue](//backissues.science.org/)
*   [Reprints and Permissions](/content/page/reprints-and-permissions)
*   [TOC Alerts and RSS Feeds](/content/page/email-alerts-and-rss-feeds)
*   [Contact Us](/content/page/contact-us)

###### [Follow Us](https://www.science.org/content/page/science-family-journals-social-media)

*   [](https://www.facebook.com/ScienceMagazine "Facebook")
*   [](https://twitter.com/sciencemagazine "Share on X")
*   [](https://www.instagram.com/ScienceMagazine "Instagram")
*   [](https://www.youtube.com/user/ScienceMag "YouTube")
*   [](/content/page/email-alerts-and-rss-feeds "RSS feeds")
*   [](https://mp.weixin.qq.com/s?__biz=MzI3NDY3NzQ2Mg==&mid=100002815&idx=1&sn=2949c025a553ac718b9612a0473b9f60&chksm=6b1120465c66a9508b01eaef1589b15d440e50b189106c8c594de8c6471f696a978de952fb15&mpshare=1&scene=1&srcid=0716JJQ5V4cKbgMMsya2MQ0n&sharer_sharetime= "WeChat")

[Get our newsletter](/content/page/scienceadviser?intcmp=ftr-adviser&utm_id=recdExfxt1yeSJxzi)

[![AAAS logo](/pb-assets/images/styleguide/aaas-logo-1672180581667.svg)](//www.aaas.org/ "Visit the AAAS homepage")

© 2025 American Association for the Advancement of Science. All rights reserved. AAAS is a partner of HINARI, AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER. *Science Advances* eISSN 2375-2548.

back to top

*   [Terms of Service](/content/page/terms-service "Terms of Service")
*   [Privacy Policy](/content/page/privacy-policy "Privacy Policy")
*   [Accessibility](/content/page/accessibility "Accessibility")

[](#)

[![ScienceAdviser](/pb-assets/images/logos/ScienceAdviser-logo-1683228767297.svg)](/scienceadviser)

Get *Science*’s award-winning newsletter with the latest news, commentary, and research, free to your inbox daily.

[Subscribe](/content/page/scienceadviser?intcmp=popup-adviser&utm_id=recbEndseGq5WulpU)[Not Now](#)

×

Back to article

![](//secure.adnxs.com/px?id=1372631&seg=23645444&redir=https%3A%2F%2Fpixel.mediaiqdigital.com%2Fpixel%3F%26pixel_id%3D1372631%26uid%3D%24%7BUID%7D&t=2)![](//ad.doubleclick.net/ddm/activity/src=8300750;type=invmedia;cat=aaas-003;dc_lat=;dc_rdid=;tag_for_child_directed_treatment=;tfua=;npa=;gdpr=${GDPR};gdpr_consent=${GDPR_CONSENT_755};ord=1?)![](//ad.doubleclick.net/ddm/activity/src=8300750;type=invmedia;cat=aaas-002;dc_lat=;dc_rdid=;tag_for_child_directed_treatment=;tfua=;npa=;gdpr=${GDPR};gdpr_consent=${GDPR_CONSENT_755};ord=1?)![](//secure.adnxs.com/px?id=1372625&seg=23645446&t=2)

\_\_("articleCrossmark.closePopup")

[PDF](https://scholar.google.com/scholar_url?url=https://www.science.org/doi/pdf/10.1126/sciadv.adu1052&hl=en&sa=T&oi=ucasa&ct=ufr&ei=R2IHaODDG_mJ6rQP7YK74A0&scisig=AFWwaeZ0DvzIWRg3ysLW0SQatW0y)[Help](https://scholar.google.com/scholar/help.html#access)